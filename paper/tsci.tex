%!TEX program = xelatex
\documentclass[fleqn,10pt]{olplainarticle}
% Use option lineno for line numbers 
\usepackage{xcolor}
\usepackage{framed}
\usepackage{xspace}
\usepackage[utf8]{inputenc}
\usepackage{fontspec}
% This section makes Fira work.
\setmonofont{Fira Code}[
  Contextuals=Alternate  % Activate the calt feature
]

\makeatletter
\def\verbatim@nolig@list{}%
\makeatother

\usepackage{listings}
\usepackage{lstfiracode}
\usepackage{url}


% For natbib, \citet for textual, \citep for parenthetical,
% \citeauthor and \citeyear.
\newenvironment{callout}
{
\begin{figure}
\begin{center}
\begin{minipage}{0.9\textwidth}
\begin{framed}
}
{
\end{framed}
\end{minipage}
\end{center}
\end{figure}
}
\newcommand{\rlang}{\textsc{r}\xspace}
\newcommand{\cpp}{\textsc{c}++\xspace}
\newcommand{\cpu}{\textsc{cpu}\xspace}
\newcommand{\nan}{\textsc{NaN}\xspace}
\newcommand{\ieee}{\textsc{ieee}\xspace}
\newcommand{\aside}[1]{\textcolor{red}{#1}}

\title{Testing Scientific Code}

\author[1]{Andrew Dolgert}
\affil[1]{IHME, University of Washington}

\keywords{software testing, unit testing, scientific software}

\begin{document}
\lstset{
  style=FiraCodeStyle,
  basicstyle=\footnotesize\ttfamily,
  %backgroundcolor=\color{green!20}
  }

\begin{abstract}
When we write code to do science, even though we have purposefully
chosen each step of the code, we may not know what the exact result should be.
Not having an exact right answer makes unit testing both complicated and
critical to trusting the scientific results. This article doesn't show how
to set up unit testing in \rlang, Python, or Julia. It doesn't cover numerical
analysis. It shows how to put boundaries on the behavior of mathematical
code. Some techniques are adaptations of traditional unit testing methods
and others are particular to testing scientific code.
\end{abstract}

% From Mike Richards
% 1. When you make parallel implementations, start with the dead-simple slowest.
% 2. Focus on moving up and down the stack, not between scientist and developer.
% You don't need to tell them everything about the subject.

\flushbottom
\maketitle
\thispagestyle{empty}

\tableofcontents
\section{Introduction}\label{sec:introduction}

The code for a scientific application, laid out across a monitor,
will have plenty of functions that look like those of any other application.
It will parse command-line flags, parse input files, and maybe even have
an event loop. Some portion, however, will look entirely different. It
may be laden with long equations, or it may be integer manipulation or
graphs, but the functions that do this work are complex, and they carry risk.

Those scientific functions are as complicated as they have to be
in order to model some feature of the world. Their math, whether mundane
or arcane, makes it difficult to use traditional unit tests to address
the risk of failure in these functions. When a model is supposed to represent
a subtle behavior of the world, testing that its output is correct feels
like an existential assertion.

However, scientific models, and the functions that comprise them,
always have behavior we can discover, characterize, and assert in a unit
test. Some of that behavior is approximate, that the result be near a
value. Some is exact, in a limit or at a boundary.

Assuming we've read a good unit testing book like \cite{jorgensen2013},
and that we can figure out, elsewhere, the construction of unit tests
in code, let's assess here the risks that are unigue to scientific code.
Let's look at how the tools we already know can address those risks
and how some new tools might help when the stakes are high.




\section{Testing Numbers}\label{sec:ieee-numbers}

The first thing that makes us call code scientific is the numbers themselves.
Floating-point math causes its own kinds of failures. Some of these are simple,
like dividing by zero or taking the square root of a negative number.
Others are surprising, like the failure of an equality test.

\subsection{Floating Point Exceptions}
I shouldn't even say that division by zero is a simple failure.
One part of it is simple, that almost every language follows the
\ieee~754 standard.
A little book called \emph{Numerical computing with IEEE floating point arithmetic}
describes both the \ieee~754 standard
and some common implications for how you write equations in code~\citep{overton2001numerical}.
The complicated part is that the application, compiler,
and even operating system act as filters to decide whether the same division
by zero will raise an exception, return infinity, or silently return not-a-number (\nan).

According to \ieee~754, the lowest level of those filters begins
as one of five exceptions that a floating-point
operation, such as multiplication or addition, can raise. These aren't the
same as \cpp exceptions, but they are what the \cpu reports when
it has a problem, and they can bubble up to your code to become a
language-level exception.
\begin{itemize}
    \item \emph{Invalid operation}---The square root of a negative
    number, or zero divided by zero. Results in a \nan, which means
    ``not a number.''
    \item \emph{Division by zero}---Results in plus or minus infinity most
    of the time.
    \item \emph{Overflow}---When an operation results in a number
    too large to represent, this can result either in infinity
    or the largest representable number, depending on settings.
    \item \emph{Underflow}---When an operation results in a number
    that isn't zero but is too small to represent, it can result
    in plus or minus zero, or the smallest representable number,
    or a special value called a subnormal.
    \item \emph{Inexact}---The result can't be represented exactly
    with the \ieee standard. Yes, this almost always happens.
\end{itemize}
In some cases, these floating point exceptions will result in termination
of a program. In other cases, the result is silently replaced
with \nan, Infinity, zero, or the smallest number, as indicated above.
When this happens is determined by your programming language,
settings in the operating system, and, ultimately, settings
in the \cpu itself.

Why would the world work this way? This \ieee standard has to guarantee
maximal speed and correctness. It does fancy tricks with rounding
and handling of very small numbers. However, it also offers
careful handling of delicate calculations.

So what I do is read \cite{overton2001numerical} and write tests
of the assumptions my code relies on.
For instance, if a Python Pandas data frame returns a \nan when it
takes a square root of a negative number, and my code relies that behavior,
then I make a test that runs negative data through the code.
\begin{lstlisting}[language=Python]
def test_f():
    df = pd.DataFrame({"a": [3.0, -3.0]})
    df.b = np.sqrt(df.a)
    assert(np.isnan(df.b[1]))
\end{lstlisting}
The risk is that some part of the software stack, from Pandas to Python, from
Windows to someone else's installation of Windows,
will make some choice that changes the behavior that I see today.


\subsection{When Are Floats Equal?}
The \ieee~754 standard is magically good at expressing real numbers as
patterns of bits. It's both fast and correct-enough. It not only represents
real numbers but also represents infinity, minus infinity, and
not-a-number, or \nan. Then there is a separate representation of
zero and minus zero, which have different bit patterns but, if you compare
them, are equal.

Comparison of \nan is a special case.
A \nan will never equal another \nan, because every equality comparison with
a \nan is false. If you ever check whether \lstinline|nan != x|, that will
always be true, but how helpful is that?
Every operation
on a \nan yields a \nan. There isn't even a guarantee that the bit
pattern that represents a \nan is the same in all cases.
One implementation embedded someone's birthday in the \nan bits.
You can, however, ask if infinity
is equal to infinity. You can also ask if
\lstinline!-0==0!, and that will be true. Rely on \lstinline!is.nan! and
\lstinline!is.infinite!, or their equivalents in your language.

Comparison of two floating points, the regular kind that aren't
infinite or \nan, has its own problem. If you set
\lstinline!x = 3.0! and \lstinline!y=9.0 / 3!, then you will probably find they
aren't equal. At least, \lstinline!x==y! will fail most times, because
floating point math is usually approximate. We test for this
using a value called \emph{machine epsilon.} Every language defines
this value. It is the least difference between the
mantissa of two numbers.

Every floating-point number is expressed as a mantissa times
an exponent. It's always of the form $1.m \times 2^e$, where
$m$ is a bunch of digits after the 2 and $e$ is the exponent
for the power of 2. There's a bit for the sign, too.
Machine epsilon is the smallest difference in the mantissa,
excluding the exponent, so if you want to test that two
floating point numbers are equal, you test relative error using
machine epsilon.
\begin{lstlisting}[language=R]
n <- 4
same <- x * (1 + n * machine_epsilon) > y && x * (1 - n * machine_epsilon) < y
\end{lstlisting}
That value of $n$ is a slop factor, but there is some sense
to it. Each time a function does another floating-point calculation,
it can introduce more drift. If you want to know whether
$3.0 = 9.0 / 3$, then you can use $n=1$, but if you want to know
whether a sequence of a thousand multiplications yields the
same result, then use a larger $n$.

For single-precision floating-point, which are 32~bits in size,
or four bytes, epsilon is near $10^{-7}$. For double-precision
floating-point, which are 64-bits in size, epsilon is near
$10^{-16}$.

In practice, when we test to see if a number is what we expect,
we're testing whether an algorithm has given the correct result.
That means there were many operations, with many rounding errors,
leading to that number. As a rule of thumb, each floating-point
operation, which is an addition, subtraction, multiplication,
or division, will add another few epsilon to the relative error.

If you're computing with doubles, and the result is off by $10^{-4}$,
is that big or small? If this is a single function you're testing,
that's probably a huge error. If it's the end of a long calculation,
or if it's the kind of calculation that divides by a small number somewhere,
then it could be fine.

This discussion is very close to a math problem called ill-conditioning,
which means that some calculations look perfectly fine on paper
but, for certain inputs, will yield wildly incorrect results due
to rounding error. We can't go into depth about that here, but notice
that a unit test for the function can see that there
is a problem with the output. It can tell you when there is a problem
that requires numrical analysis.


\subsection{Factors Are Many Parameters}
\aside{Seems should go in section 5.}
We talked about combinatorial testing of parameters
in Sec.~\ref{sec:parameter-logic}. That section was talking about
parameters that are boolean, or ternary, or one value of ten.
A double can take on about $10^{19}$ different values. A single
can take on about $10^9$ different values. We can't treat them
like other parameters.

Except that there are some cases where a small function,
such as calculating Hilbert curves or quadtrees, is fast-enough
that you can run through every single binary representation of
a single-precision floating point. Thats $2.1\times 10^9$ floating
point values at $3\:\mbox{GHz}$, for a function that takes
$\sim 100\:\mbox{clock cycles}$. That comes out to $71\:\mbox{seconds}$
to try every possible number. This does not work for doubles.

More practically, for a single number you have to look at
the specific problem and identify edge cases for that number.
Maybe the edge case is near zero or one. Put on your scientist
hat, and make guesses. Then test two things: the value at the
edge case and the value approaching the edge case from positive
and negative sides.

If a parameter is a vector of numbers, what to test depends
even more on the particular math. Some suggestions:
\begin{enumerate}
    \item Set all values to a single edge case.
    \item Try monotonically-increasing or decreasing.
    \item Try non-increasing or non-decreasing (so that some
    consecutive values are the same).
    \item End points may be special, so test them separately.
    \item If the vector is, in some way, smooth, construct
    a vector where the maximum change, element to element, is no
    more than some prescribed value. See how large that value can
    be before the function fails.
\end{enumerate}
\aside{Stratify domain.}



\section{Statistics for Testing}\label{sec:statistical}

\subsection{Randomized calculations should give different answers each time}

A simple Gaussian elimination can benefit from random pivoting.
A function to sample a distribution is supposed to return a new value
every time it's called.
Some functions introduce random reorderings
so they can insist that the caller not expect a given order
in the output. All of these perfectly useful functions
thwart using simple lookup tables to test their output.

The tests, themselves, can be the problem when we randomly
generate test cases. I'll check
a life expectancy function against a corpus of life expectancies
for every country. I'll randomly generate user input
to see that a functions succeeds or gracefully fails.

When testing these random functions,
we have on our side some basic tools
of statistics. Some, like mean square error, are ways
to heap our uncertainty into a single, tidy number.
Others are formal statistical
tests. They take a clear side-effect of a function,
its randomness, and tame it to measurable uncertainty.
Let's look at just a few examples.

\subsection{Approaches}
\begin{figure}
    \centering
    \includegraphics[scale=0.25]{gaussianlimits.pdf}
    \caption{Gaussian with limits}
    \label{fig:gaussian-limits}
\end{figure}
\subsubsection{Error estimation}
When the function under test returns a vector of values,
and I want to compare it with a vector of known values,
I use an estimator, like mean-squared error, of the difference
between the two.

The mean-squared error between $N$ observed values $\hat{Y}$ and $N$ known values $Y$ is $\frac{1}{N}\sum_i (Y_i - \hat{Y}_i)^2$.
It takes the whole set of values and reduces it to one number I can assert must be less than some value.

When values range from small to large, it's convenient to use
relative error instead of absolute error.
\begin{equation}
  \mbox{relerr} = \frac{\hat{Y}_i - Y_i}{Y_i}
\end{equation}
The known value is always in the denominator.

Even more reassuring for a unit test, I can take all of the
sets of values and take the maximum absolute error,
$\mbox{max}_i |Y_i-\hat{Y}_i|$. That's a guarantee there isn't
some boundary value whose crazy-high error is hidden beneath
low error everywhere else.

\subsubsection{Variance, standard deviation}
When there is just one vector of random numbers coming out
of a function, it can be helpful to characterize how far
they are from their average value. The variance is
the average of how far each value is from the average of all
values,
\begin{equation}
  \mbox{var}(Y) = \frac{1}{N}\sum_i (Y_i - \bar{Y})^2,
\end{equation}
but you'd use the always-avaiable
as \lstinline!var! or \lstinline!variance! in any language.
The standard deviation is the square root of variance.

If variance is zero, then every value in the vector is
the same. There are some functions, such as the Cauchy
distribution, that don't have a variance. For such a function,
the value of the variance will keep increasing as the
number of draws of the function increases, instead of
converging to a single value. This, in itself, is a
testable property of a function.


\subsubsection{Confidence interval}

Assume there is a function with randomness, that produces floating-point numbers. Those results
have some average value, but they vary around that average value.
If we run this function a thousand times and take the
lower value, below which 2.5\,\% of the results fall,
and the upper value, above which 2.5\,\% of the results fall,
then this interval is the 95\,\% standard deviation.

For research statistics, the standard deviation is often part
of an argument that a model of a problem is appropriate to
the observed data. For unit testing the standard deviation
is a rule of thumb for how often a perfectly good unit test
will randomly fail. If you want it to fail five percent of
the time, use a 95\,\% interval. If you want it to fail
one time in a thousand, use a 99.9\,\% interval.

Given that results from a function are random, and we
want to make some cutoff on which results are acceptable,
we want to choose a cutoff that is informative, so it's
close to the average good value. On the other hand, we don't
want it to fail a lot. The standard deviation tells us
how much push luck in either direction.

\subsubsection{Pearson coefficient}
Let's say the output of a function is a set of $y$ values,
and the input is a set of $x$ values. You may not know
how $y$ depends on $x$ exactly, but you know that it
does. The Pearson coefficient is a function of
$N$ $y$ values and $N$ $x$ values that will return a number
between -1 and 1, where -1 is a line with a downward slope,
1 is a line with an upward slope, and 0 means the points
are evenly arranged so that they don't slope up or down
and aren't collinear.

For a single $y$ that depends on a single $x$, the equation
is a few lines of code.
\begin{equation}
r_{xy} = \frac{\sum_i (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_i (x_i - \bar{x})^2}\sqrt{\sum_i(y_i - \bar{y})}}
\end{equation}
It conveniently generalizes to more dimensions.

The Pearson coefficient is a handy, if imprecise,
set of guard rails on the behavior of a function.
It quickly determines whether the output rises or falls with the input,
even in the presence of noise. It can say whether the output
isn't linear enough, or whether it's too linear.
It even has a multidimensional generalization for data that
has multiple $x$ and $y$ values.


\subsection{Taming randomness}

I make tests with random inputs, so they explore more alternative
paths in the code than I might guess. I run tests of random
functions a thousand times so that they have more chances to
show a real bug. Randomness in testing is a free source for
finding faults, and it's a constant headache.

In order to use the statistical tests above, I need to choose
a cutoff on what is a passing test and what is a failing test.
If I choose a cutoff that's useful for finding faults, it will
always fail for some percentage of tests. For a 95\,\% confidence
interval, it will fail 5\,\% of the time. Even for a
99.9\,\% confidence interval, it will fail once in a thousand
tries. This means the green lights of a unit test display will
be red without reason once in a thousand times, but that's
enough not to pay attention to a red light. It's a lure to
rerun the test suite every time there is any red light.
It's a waste of attention.

The solution is simple. Fix the seed for the random number
generator for each test that has randomness. The random
seed determines the exact set of random values that will
follow. If the test passes for a particular random seed,
then it will always pass for reruns of the unit test until
some fault is introduced.

The simple solution is sometimes impossible and always
disappointing. Most programming languages offer the tremendous
convenience of calling \lstinline!rand()! anywhere in the
code, which uses a global random number generator. Sometimes
\rlang code calls \lstinline!rand()! and then calls \cpp code
that calls \lstinline!rand()! from a different random number
generator. Authors
of code unfortunately use this convenience, creating side-effects
for testing. Only sometimes can we make a repeatable
stream of random numbers by declaring a specific random number generator
and passing it to a function.

(It's also possible to greatly reduce the rate of failure
for a statistical test, even with the same 99\,\% cutoff
on acceptable results, if we take the same hundred tests
and use the statistical framework to treat them as ten sets
of ten tests.)

Let's say we can fix the seed for a function, and for all the
functions it calls. The joy of randomized testing is that more
runs might yield more faults in the code. We
can still do lots of runs by creating a unit test that is run once
for a set of seeds.  The procedure is to define a starting seed and run against
the next thousand seeds. When the test shows no real faults,
choose a starting seed for which the next thousand all pass
the test within some tolerance. This is less simple, but it
explores the space of tests and is likely to signal when
a real fault arrives.

\subsection{Conclusion}

Some kinds of unit tests are absolute. When they pass,
there is no fault in this one path through the function.
Statistical tests don't give that guarantee. The function
under test could be close-enough to the desired value but
still have a fault in some code path.
At the same time, statistical tests can find faults
in places you weren't looking, because they summarize all
of the outputs of a function or because they summarize
the behavior of multiple runs of a function.

If we were testing a typical function, for which we knew the
right answer, that unit test would offer the meager assurance that,
for one set of parameters, the function is correct.
Statistical tests, like those above and many more statistical
tests not mentioned, can assert that, for one set of parameters,
a function isn't too wrong.


\section{Parallel Implementation}\label{sec:parallel-implementation}
\subsection{Write it Twice}
Read through unit tests, and they all have the same form.
They set up data and parameters to pass to a function, call the
function, and either compare its output to some known answer or to
something known about the answer. Every unit test is two
functions, the function under test and a parable
of that function, written into the unit test itself.

There are times that it's reasonable to write a test version
of a function, some second, similar way. In fact, we most often write
a function first using its most obvious, probably slowest, algorithm.
The obvious algorithm is an excellent unit test on any refinement.

We can find an appropriate parallel implementation for testing by
looking at the set of choices we make when writing the original function
and asking, at which step should the second implementation differ
from the first.
\cite{dahlgren2005} describe three steps to writing a function for
scientific software, and we'll add one step before theirs.
\begin{enumerate}
  \item Represent the mathematical model.
  \item Prepare discrete models.
  \item Translate the models into algorithms.
  \item Code the models using programming languages.
\end{enumerate}
If only the algorithm in the code changes, then the results
should be quite similar. If the models are discretized differently,
then it takes more care to use one for testing of the other.
Which you choose depends on the kinds of faults you think pose
the most risk in the code.

The benefit, and threat to efficiency for testing, is that
parallel implementations of a function can lead to scientific
questions. Good unit testing strategies limit testing to the
highest risks in the software, and that principle applies
especially to use of parallel implementations, so let's
look at how alternative choices probe faults in code.

\begin{callout}
\textbf{The Oracle Problem}---
Imagine that there were some source to tell you what the answer to a function
should be. It could be a software engineering support tool, or it could
be your scratchwork on a notepad. This source is known as an
oracle~\citep{howden1986functional}, and its absence for scientific code
is the oracle problem~\citep{sirer1999using}. Despite the name, this
is hardly a tragedy because testing methods can 
finding simpler inputs, compare results with another implementation,
or check properties of the function without checking the function itself.
\end{callout}


\subsection{Brute force numerical algorithms for comparison}

Let's say we are supposed to calculate the mortality rate
for five-year age groups from birth to eighty years old,
for a Gompertz-Makeham mortality law. The parameters are
$(x, \alpha, \beta, \lambda),$ intermediate values are $(\mu, S)$ and the result is ${}_nm_x$.
\begin{eqnarray}
  \mu(x) & =&  \alpha e^{\beta x} + \lambda \\
  S(x) & = & \exp\left(-\lambda x - \frac{\alpha}{\beta}\left(e^{\beta x} - 1\right)\right) \\
  {}_nm_x & = & \frac{\mu(x+n) S(x+n) - \mu(x)S(x)}{S(x) - S(x+n)}
\end{eqnarray}
Wherever this equation comes from, whether it be a book or a
\textsc{png}\ in a Jira story, we can identify risks in transliterating
these equations to code. The most likely faults are improperly grouping
algebraic operations, mistaking signs, and forgetting terms. 

We can address those risks by returning to the first step of writing
a scientific function, which is how we represent the mathematical model.
The equations above are the result of integrating a function that has fewer
steps, even though it would be slower to run.
\begin{eqnarray}
  \mu(x) & =&  \alpha e^{\beta x} + \lambda \\
  {}_nm_x & = &\frac{\int_{x_n}^{x_{n+1}}\mu(s) \exp(-\int_{x_n}^s\mu(a)ds)ds}{\int_{x_n}^{x_{n+1}}\exp(-\int_{x_n}^s\mu(a)ds)ds}.
\end{eqnarray}
The resulting ${}_nm_x$ should be the same as that above within
machine precision, where machine precision means that we expect a drift
of about one machine epsilon for each step of the integration. That means
it should agree within several thousand times machine epsilon.

This new version could have its own faults and its
own corner cases, but they are unlikely to be the same faults
as the original function. This is a tremendous improvement over
not having such a test.

There are many possible parallel implementations. For this example,
we could make the looser assertion that the relative error of ${}_nm_x$
to $\mu(x)$ should be less than some cutoff. It's not as strong a test,
but it's quick to write.

What to test with brute force computation, and how to brute
force that computation, depends on risk assessment, time, and
whether your work is to question faults in the code or faults
in the model.



\subsection{Another language or library}

There is something lovely about the task of rewriting code from
one language to another. On one side sits working code, problems
solved. On the other is a green field for expressing it in the idioms
of a new language. The original code is a gift, not only as a guide,
but as a set of unit tests for the new code, but only for those
willing to take the painful step of limiting their first translation
to perserve structure around the riskiest functions.

Most languages provide some built-in interoperability to call
other languages. R, C++, Julia, and Python play together like hamsters
in a habitrail. It's perfectly reasonable to keep code from
a different language in with the suite of unit tests and to
call those functions when the interoperability is available.

Testing across languages, or in the same language on different
operating systems, can probe a unique set of problems. Data types
change, revealing limits to numerical accuracy and inappropriate
assumptions about data structure. Libraries change, the ones that
provide special functions or system services. For instance, the
generalized inverse in Python will return a different value
from the generalized inverse in R. When this breaks tests, it
clarifies which, of the four kinds of generalized inverse,
the code needed, and that there should be a test to see which
is available.


\subsection{A figure in a paper}
If you read about an algorithm in a paper, and that paper has a table or figure,
you could check your results against the values in the table or figure.
Fig.~\ref{fig:wilson_chart} shows a chart from a paper. The unit test
uses those figures.

\begin{lstlisting}[language=R]
wilson_score_interval <- function(p, n, confidence) {
  z <- qnorm((1 + confidence) / 2)
  fixed <- p + z**2 / (2 * n)
  shift <- z * sqrt(
    (p * (1 - p) + z**2 / (4 * n)) / n
  )
  denominator <- 1 + z**2 / n
  list(lower = (fixed - shift) / denominator,
       upper = (fixed + shift) / denominator)
}

paper_numbers = data.frame(list(
  n = c(
    1, 1, 6, 15,
    1, 6, 7, 13, 3,
    4, 4, 18, 4,
    3, 20, 49, 18,
    13, 59, 16),
  p = c(  # "Total n" column
    1, 1, 0.8333, 0.4667,
    0, 1, 0.4286, 0.5385, 1,
    .5, .75, 0.6667, 0.5,
    1, 0.6, 0.5306, 0.6111,
    0.3846, 0.3898, 0.5),
  w_minus = c(  # "p(shall)" column
    0.2065, 0.2065, 0.4365, 0.2481,
    0, 0.6097, 0.1582, 0.2914, 0.4385,
    0.15, 0.3006, 0.4375, 0.15,
    0.4385, 0.3866, 0.3938, 0.3862,
    0.1771, 0.2758, 0.28),
  w_plus = c(  # "D:w-" column
    1, 1, 0.9699, 0.6988,
    0.7935, 1, 0.7495, 0.7679, 1,
    0.85, 0.9544, 0.8372, 0.85,
    1, 0.7812, 0.6630, 0.7969,
    0.6448, 0.5173, 0.72)
))


test_that("paper numbers match computed", {
  confidence <- 0.95
  result <- wilson_score_interval(
    paper_numbers$p, paper_numbers$n, confidence
  )
  tolerance <- 0.001
  for (i in 1:length(result)) {
    expect(abs(result$lower[i] - paper_numbers$w_minus[i]) < tolerance,
           paste("no match:", result$lower[i], paper_numbers$w_minus[i]))
  }
})
\end{lstlisting}

\begin{figure}
    \centering
    \includegraphics[scale=0.4]{wilson_chart.pdf}
    \caption{A paper about confidence intervals published a chart of values for
    the Wilson score interval~\citep{wallis2013binomial}, in the last two columns. Using $n$ and $p$
    for a $95\:\%$ confidence interval, we can quote this chart and compare with
    our code's output as a unit test.}
    \label{fig:wilson_chart}
\end{figure}

\subsection{Regression tests}
A regression test compares the result of a function against a previous
result from that function. The parallel implementation, in this case,
is the previous version of the same function.

There is a book called \emph{Release It!} that says we should
``make regression testing easy,''~\cite{nygard2018release}. It's hard to make it easy
because unit tests are built to test, not save output. Even if
they save output from functions, it takes work to figure out
\emph{how} a result changed if it did change. If the result
is a data frame, did every value change, or some subset?

While following the book's advice is a research project,
it's a good focus for any particular project to find places in the
code where regression tests would be most informative. Unit tests are
most absolute in their pronouncement when they test the simplest
functions. It's the mid-level functions, that do more work
and produce the more complex output, that benefit from regression.
The high-level functions will change output every time you touch
the code, but mid-level functions will change less frequently,
and there's a chance to guess why they changed.



\section{Testing parameter logic}\label{sec:parameter-logic}
\subsection{Introduction}
If we need to implement some matrix algorithm because of some structure
in our computation, and it's not already in a major library, then
we might make a function that has a few arguments, the matrix, algorithm parameters
to control cutoffs in the algorithm, and options to determine whether
the result is written back into the source matrix memory.
The data, parameters, and options are all, in the eyes of unit testing,
parameters.

These three kinds of parameters, however, tend to affect functions
differently. Options appear in the test of if--thens, so they change
the code path. The code path is the sequence of blocks of code that execute
for a particular set of parameters.
Algorithm parameters tend to appear in key equations, so that they
have limited effects on data flow in a function. The data flow is a graph
of which values in a function depend on which previous values.
The input matrix, itself, is effectively a two-dimensional set of
parameters, greatly increasing the number of possible inputs to a function.


\subsection{Decision Tables}

Given that scientific functions have large spaces of possible inputs,
we have to choose which parameters and parameter
combinations to test, and we limit the tests by identifying
parameter values that are nearly-the-same test. For instance,
if a function calculates age-standardized mortality rate, then testing
the case where the mortality rate is 1.1 times larger hardly seems
a different test. Instead, we might ask how the function handles
missing values, zero values, or unusually sharp rises or falls in
mortality rate. Sets of parameters that probe the same faults in
the function under test are called \emph{equivalent.}

Methods to test parameter logic all have the same underpinning,
that they will try to test at least one set of parameters from
each of the equivalence classes of inputs. Deciding what is an
equivalence class can be difficult. Sometimes two parameters
sets are equivalent in terms of the mathematical statement of 
the model, but their representation in code makes one set
have a different risk from another. For instance, a function of small
values would be well-defined as an equation, but it could cause
denormalization when written in code.
It can also be the other way around, that
the mathematical statement of a model makes clear what parameter
sets may be equivalent. Information from both statements of
a function will be helpful.

A decision table enumerates sets of inputs and desired outputs for each set.
For example, if a variable can be 0, 1, or 2, then there are three possible
inputs. The chart would be three lines long. If the function were $2^n$,
the chart might be

\begin{center}
\begin{tabular}{|l|l|}\hline
power & output \\ \hline
0 & 1 \\
1 & 2 \\
2 & 4 \\ \hline
\end{tabular}
\end{center}

If we add a parameter that says whether the variable is positiver or negative,
then the decision tree has $3\times 2$ rows,

\begin{center}
\begin{tabular}{|l|l|l|}\hline
power & positive &output \\ \hline
0 & T & 1 \\
1 & T & 2 \\
2 & T & 4 \\
0 & F & 1 \\
1 & F & 1/2 \\
2 & F & 1/4 \\ \hline
\end{tabular}
\end{center}

This is great, but the list gets big because the options multiply.
In the example here, we knew the exact answers.
When there are more cases, you may only be able to \aside{specify properties} of the
answer as a function of the parameters. It's a weaker check,
but it could help.

\subsection{Limiting values of parameters}\label{sec:limits}

Not every parallel implementation is an brute-force retelling
of a model. Most mathematical functions provide finger-holds
support of their correctness.

The simplest of these are parameters for which a function
takes a known value. The function could be a mess of
polynomials of trigonometric quantities,
\begin{equation}
  y = (\sin x)^3 - 6 (\cos x)^2 + 2,
\end{equation}
but these simplify considerably at $x=0, \pi/4, \pi/2, \pi,$
and so on. If the function should divide by zero for some
number, then that, too, is a parameter for which the function
has a known value, an exception.

Sometimes a limit of a function is the same limit you know
from high school precalculus,
\begin{equation}
  \lim_{x\rightarrow\infty} f(x).
\end{equation}
Maybe the limit goes to zero, or to 3, but the principle is
the same. You can test that a function is near the limit
as the value gets larger, or closer to zero. Better, you can
test that the function gets closer to the limit as the
parameter gets closer to its limit.

This is a game where incremental understanding of math
gives incrementally more careful attention to risk in the
function under test. We can know the limiting value of a
function, but we can also know how the function approaches
that limit. Another precalculus friend, the Taylor series
writes a function as a sum of terms that get smaller,
\begin{equation}
  f(x_0+x) \approx f(x_0) + \frac{f'(x_0)}{2}x + \frac{f''(x_0)}{6}x^2 + \epsilon.
\end{equation}
If we know the first term in the series, then the second term
is a good guess for how far the function's output should be from
the limiting value, as a function of $x$.


\subsection{Test Case Generation}
\aside{By-hand versus complete cases}
Could we tell the testing framework about our input parameters
and let it do \emph{automatic test case generation?}
If we test a function by supplying every combination of inputs,
it's called \emph{combinatorial interaction testing.}
The number of tests generated can be large, even for few parameters.
That can make it difficult to supply all of the expected results.
If we could exclude parameter combinations that definitely fail
or are uninteresting, there would be many fewer tests.

The fraction tested, of all possible inputs, is \emph{test coverage.}
If test exercise every possible input for each parameter, separately,
that's depth-1 coverage. If all pairs of inputs are tested, that's
depth-2 coverage. It's usually good to prioritize lower-depth coverage
before higher-depth coverage. What are the main practical
approaches to generating tests with good coverage up to some depth?

\subsubsection{Random Parameter Choices}
Given a set of possible values for each parameter, use a random
number generator to choose one of each. Do this as many times
as runtime allows. (Can I find expected coverage as a function of
random draws?) The Python tool, \texttt{Hypothesis,} uses this
method. \aside{example}

\subsubsection{Combinatorial Excursions}
If you decide that there is some most-common way to call
the function, set that as the baseline. Then loop over parameters,
trying values for each one. Then loop over pairs of parameters.
Then sets of three. These tests would be highly-biased
towards that baseline parameter set. \aside{example}

\subsubsection{All Pairs}
There may be software available to generate tests which
include all pairs of input parameters.
I suggest using a library because generating tests that have
all pairs can be tricky to do well.
The Pairwise website lists libraries in
various languages~\cite{Pairwise}. For instance, AllPairsPy
does this for Python~\cite{allpairspy}.
An example all-pairs algorithm is in~\citet{tung2000automating}.
How to write such an algorithm is covered in professional
textbooks on software testing, such as~\citet{pezze2008}.
Two Microsoft articles might be a friendly start to writing
one yourself~\citep{blass2002,czerwoka2006}.

\aside{example, and describe limitations}


\subsubsection{Stratification and Filtering}
It helps a lot to select subsets of the parameter
space that must be run or that can be ignored. This work
is largely done by hand. Maybe you set the value of one parameter
and then use an automated tool to generate the rest of
the parameters given that one value. This stratification
can halve the number of tests without increased risks.
Python's Hypothesis helps build stratified tests.

There are also two kinds of filtering you can do on 
generated test cases. You can exclude tests for combinations
of parameters that violate preconditions of the function.
You can also filter out tests that you feel are less risky.
These latter, soft conditions, make sense when you can
look inside the code in order to assess risk.

Together, stratification and filtering reduce the total number
of tests to run so that they focus on the highest risks
in the shortest time.

\subsubsection{All Other Methods}
If each run of a test takes a long time, it might be worthwhile
to choose tests yet more wisely than any method listed above.
To understand the problem, I recommend reading~\citet{petke2015practical}.


\emph{Orthogonal arrays} are a way to construct random draws
of parameters that can be shown to be more spread out through
the space of all possible draws. Generating orthogonal arrays
appears to be difficult, so people start by looking up the
needed array in a repository and then use that to generate
test cases~\citep{Owen1992}.

There are a couple of libraries or web services that offer
to generate test cases. \textsc{aetg} uses a greedy algorithm~\citep{cohen1997aetg}.
The automated combinatorial test methods (\textsc{act}) library
is also greedy~\citep{kuhn2008automated}.
The combinatorial testing services library (\textsc{cts})
uses simulated annealing to find good test cases~\citep{hartman2004problems}.

There are survey papers on the topic~\citep{nie2011survey,khalsa2014orchestrated}.


\subsection{Example}
This should use Wilson's Score Interval to guess standard error for a measurement~\citep{agresti1998approximate}.

This function finds standard error from effective sample size for two
different models. One is the binomial model, denoted ``proportion,''
and the other is a demographic rate, denoted ``rate,'' which is not bounded
by 1. For the binomial model, the Wilson score interval is
\begin{equation}
    \frac{\hat{p}+z^2/2n\pm z\sqrt{\frac{\hat{p}(1-\hat{p})+z^2/4n}{n}}}{1+z^2/n}
\end{equation}
where $z=z_{\alpha/2}$, so it's .975 for a $95\:\%$~confidence interval.
I think it will translate between confidence interval and standard
error by assuming asymptotic normality, which means assuming the
confidence interval is
\begin{equation}
    p\pm z \times\mbox{standard error}.
\end{equation}
This means a standard error from Wilson's score interval comes from
\begin{eqnarray}
    z \times\mbox{standard error} & = & \frac{z\sqrt{\frac{\hat{p}(1-\hat{p})+z^2/4n}{n}}}{1+z^2/n} \\
    \mbox{standard error} & = & \frac{\sqrt{\frac{\hat{p}(1-\hat{p})+z^2/4n}{n}}}{1+z^2/n}
\end{eqnarray}

For the standard error of a rate, there are two calculations, an asymptotic one
for large effective sample size and another for small effective sample size.
For large sample sizes, standard error is $\sqrt{\lambda/n}$, as expected for Poisson
rates. For small sample sizes, this code uses a linear interpolation between two values,
which are the large sample size value, pinned to $\lambda n=5$, and the value $1/n$.
\begin{equation}
    \left(1-\frac{\lambda n}{5}\right) \frac{1}{n} + \left(\frac{\lambda n}{5}\right)\sqrt{\frac{5/n}{n}}.
\end{equation}
A larger selection of standard estimators for Poisson rates is in \citet{patil2012}.

%https://stash.ihme.washington.edu/projects/CC/repos/cascade_ode/browse/cascade_ode/drill.py
\begin{lstlisting}[language=Python]
def se_from_ess(p, ess, param_type, cases=None,
                quantile=0.975):
    """ Calculates standard error from effective sample size
    and mean based on the Wilson Score Interval """

    err_msg = ("param_type must be either "
               "'proportion' or 'rate'")
    assert param_type in ['proportion', 'rate'], err_msg

    if cases is None:
        cases = p * ess

    if ess == 0:
        return 0

    if param_type == "proportion":
        z = stats.norm.ppf(quantile)
        se = np.sqrt(p * (1 - p) / ess + z**2 / (4 * ess**2))
    elif cases <= 5:
        # Use linear interpolation for rate parameters
        # with fewer than 5 cases
        se = ((5 - p * ess) / ess + p *
              ess * np.sqrt(5 / ess**2)) / 5
    elif cases > 5:
        # Use Poisson SE for rate parameters with more than
        # 5 cases
        se = np.sqrt(p / ess)
    else:
        raise Exception(
            "Can't calculate SE... check your cases parameter")

    return se
\end{lstlisting}

\begin{center}
    \begin{tabular}{ll}
    \hline
    Variable & Possible Values \\ \hline
    mean $p$ & $p<0$, $0<p<1$, $p=0$, $p>1$, single value or array \\
    effective sample size $e_{ss}$ & $e_{ss} <0$, $0<e_{ss}<p/5$, $e_{ss}>p/5$ \\
    param type & proportion, rate, other \\
    cases & missing, None, negative, $<5$, $5$, $>5$ \\
    quantile & missing, $0<q<1$, $q=1$, $q>1$ \\ \hline
    \end{tabular}
\end{center}
The decision table will be quite large, but we can start with some
particular values.
\begin{center}
    \begin{tabular}{llllll}
    \hline
    mean & ess & param type & cases & quantile & result \\ \hline
    0.04 & 100 & rate & missing & 30 & $0.02$ \\
    0.04 & 100 & None & missing & missing & assertion \\
    0.04 & 0 & rate & missing & missing & 0 \\
    (0.04, 0.01) & (0, 0) & rate & missing & missing & (0, 0)
\end{tabular}
\end{center}

This function is written to handle an array of mean values.
If the input mean is an array of numbers and the standard error
is zero, the output of the function is a single zero, not an array
of zeroes.

\subsection{Conclusion}
If you can separate parameter handling from the rest of the code,
you can test better. The idea is to transform from input parameters
that are user-friendly to those that are simpler for the algorithm
to use.
Think of parameter-handling as translation from an external
     language to some kind of parameterization that's more
     consistent, concise for internal processing.


\section{Random changes to the code}\label{sec:random-changes}
The function under test, as with any block of code, can be
represented as a syntax tree. This tree of operators, operands,
and assignments, represents a contract with the compiler,
represents the flow of information from parameters to output,
represents a treasure map of potential faults. There is a form
of testing that reads the function under test, creates its
syntax tree, and randomly mutates it. It adds a one to the
limit of a loop, and runs the unit tests again. It changes
the order of assigment statements and tests again. Each time
it records the change to the tree, and the outcome of the tests.

When the test suite is mature, then there should be some test
that fails for each mutation, but that isn't the case. There
will be twenty percent of genetic mutations of the code that
cause no test to fail, and most of those won't indicate the
code has a fault, or even that tests are incomplete. Instead,
these cases force a question about what choices in the code
matter and what is irrelevant.

One way to measure what matters in code is to pick a
parameter and mark every line where its value affects
an assignment. Then track each of those down the line.
You'll see that only under certain conditions do certain
parameters change a result.

A similar way to understand a function is to choose central
moments, the lines where the important math happens, and
trace back to what affected those lines and forward to
how those lines become the result.

These are lenses into the code that clarify relationships
among corner cases, dependence and independence from
line-by-line choices. The lens is more important than the
mutation technique, which should be left to those with
undue free time or responsibility for manned rockets.


\section{Testing data movement}\label{sec:data-movement}

\subsection{Data movement}

We said earlier that there are three steps to making scientific
software, choosing the mathematical model, discretizing that
model, and deciding algorithms to implement the discretized model.
Rarely is scientific software just one mathematical model
and just one ensuing algorithm. It's a series of models supported
by a series of algorithms.

For this software, as for any software, using the right algorithm
ensures speed and scalability, and the right algorithm depends on
its associated data structure. The relevant undergraduate class
is called Data Structures and Algorithms, always the two together.
As a result, performant scientific code is a series of algorithms
joined by a series of translations from one data structure to
another, each to support the next algorithm. The high-performance
computing crowd calls any such code \emph{data movement,} as though
it were excreted, but software engineers call it integration code
and call the testing of it integration testing.

\subsection{Integration testing data movement}

A classic example of data movement is reorganizing a multidimensional
array from one axis ordering to another. An algorithm with nested
for-loops usually acts on nested axes. The order of the axes should
put those for the inner loops last (for C and Python which are row-first), or put those
same axes first (for R and Julia which are column-first). This so
often controls the speed of an algorithm that it isn't considered an 
optimization to fix it but an error to fail to set the order.

More complicated an example would be translating a graph from
an edge-list representation to an adjacency matrix representation.
Both have their place, but the correct form depends on the
algorithm that will act on this data.

The question isn't how to test data movement. It's that data
movement is separate from the algorithm. It should be a separate
function and tested on its own. It has its own set of faults,
that center around integer arithmetic, the off-by one errors,
the loop endpoint errors. These are different in nature from
the taxonomy of faults that plague an algorithm-in-itself,
but they are no less debilitating to correctness.

\subsection{The CRUD case}\label{sec:crud}

The prosaic brother to data movement is the ubiquitous selection
and editing of data frames we do when reading and writing data.
The data frame, in which this data typically lives, is the
algorithmic equivalent of a minivan, fully equipped but less
computationally intriguing.

Dataframe manipulation for loading and saving data is more a matter
of how we want to specify a program than about moving from
one data structure to another. There is, happily, a framework
for recording and conveying this work, the create-read-update-delete
(CRUD) list.

\begin{itemize}
\item \textbf{Create} - If the input data doesn't have excess mortality,
  then derive it from cause-specific mortality and prevalence.
\item \textbf{Read} - If the user specifies \lstinline!get_csmr!, then get cause-specific
  mortality from the database and add it to the data.
\item \textbf{Update} - Use the average of incidence data at all child
  locations because it tends to be sparse.
\item \textbf{Delete} - All-cause mortality isn't included in this step.
\end{itemize}

These are requirements. Failure to meet one of these requirements
often won't make the code stop running. It will harm confidence
in the code.

These tests are therefore classic boundary-value tests, where
we try parameters at the lower boundary, upper boundary,
and middle of the range. Combinatorial testing can be appropriate
across all parameters. What isn't mentioned in Jorgensen, for instance,
is that input datasets are parameters. They are high-dimensional
parameters which can make combinatorial testing quite large.
You are asking not only whether the dataset is nonzero in length,
as a boundary value, but whether each kind of data record is at
any boundary value that it can take on. For all cases, the goal
is to assert that requirements, as stated above in CRUD categories,
are true for all parameter values.

\subsection{Example}
This example, in Python, is a function with two arguments, but both
are dataframes, just read from input files. Think of them as two spreadsheets
with multiple rows and column headers ``x\_local'', ``a\_data\_id''
and so on.
\begin{lstlisting}[language=Python]
def shift_priors(self, data, adj_data):
    """For subnationals, shift priors given parent run.

    Args:
          data (pd.DataFrame): All of the input data, including priors.
          adj_data (pd.DataFrame): Parent predicted data values.

    Returns:
        Data with adjusted priors.
    """
    integrands = data.integrand.unique()
    priors_from_draws = data[data.x_local == 0]
    unadjusted_data = data[data.x_local == 1]
    parent_predict = adj_data[
        (adj_data.x_local == 1) &
        (adj_data.a_data_id != IHME_COD_DB) &
        (adj_data.location_id == int(float(self.loc)))]
    shifted_priors = pd.DataFrame()
    for ig in integrands:
        ig_prior = priors_from_draws[priors_from_draws.integrand == ig]
        ig_data = parent_predict[parent_predict.integrand == ig]
        if len(ig_prior) == 0:
            pass  # No priors to adjust
        elif len(ig_data) == 0:
            # No fit with which to adjust, so keep the priors.
            shifted_priors = shifted_priors.append(ig_prior)
        else:
            ig_prior -= ig_data.mean()
            shifted_priors = shifted_priors.append(ig_prior)

    shifted_priors['meas_value'] = shifted_priors.meas_value.clip(lower=0)
    newdata = unadjusted_data.append(shifted_priors)
    return newdata
\end{lstlisting}

If we wanted to make a decision table for this function, what are
the possible values of each entry in each row of the input data?

\begin{center}
\begin{tabular}{|l|l|}
\hline Variable & Values \\ \hline
\lstinline!data.x_local! & 0, 1, not 0 or 1 \\
\lstinline!data.integrand! & same as another row, same as adjusted row \\
\lstinline!adj_data.x_local! & 0, 1, not 0 or 1 \\
\lstinline!adj_data.a_data_id! & \lstinline!IHME_COD_DB! or not \\
\lstinline!adj_data.location_id! & \lstinline!self.loc! or not \\
\lstinline!adj_data.integrand! & same as another row, same as data row \\ \hline
\end{tabular}
\end{center}

That makes $3\times 4 \times 3\times 2\times 2\times 4=576$ different inputs.
This excludes the effects of calling the function
\lstinline!adjust_prior_by_mean_of_fit()!.

What is the algorithmic complexity of this function? There is one
if--then--else, so the answer could be three, but each condition in a dataframe
selection is another path through the code. For instance,
\lstinline!data.x_local == 0! is a conditional with two outcomes.

A project manager can ensure good communication between the scientist and
developer by writing \textsc{crud} assertions in English.
\begin{itemize}
    \item[R1.] Read parent location's estimate of priors as ``adjusted data.''
    \item[U1.] Update adjusted data so that the mean of the priors for this location matches
        the mean of the data for this location. Keep the shape of the priors.
    \item[U2.] Update priors, after adjustment, so that they are greater
        than zero for all integrands.
    \item[D1.] Delete local adjusted data that doesn't match the current location.
    \item[D2.] Delete local adjusted data that doesn't match the integrands.
    \item[D3.] Delete local data for this location if there are no parent
        priors for the same integrand.
\end{itemize}
Then we can write tests that verify these \textsc{crud} requirements for
each of the 576 kinds of inputs. \aside{show the test}


\subsection{Trust the movement}

In order to test data movement separate from the main algorithm,
we have to separate data movement from the main algorithm.
This is one of those times that what is testable indicates what
is modular in code.


\section{Using symmetry to test a function}\label{sec:symmetry-test}

Sometimes knowing the math of a software function lets you
test the software, even when you have an oracle problem.
\emph{Metamorphic testing} uses the symmetry of the math function
in order to test the correctness of the software function~\cite{ding2016,guderlei2007,kanewala2015,liu2014}.

The mathematical description of a function is that it relates
a domain (input) to a range (output). We write $f(x) = y$.
If the function is an exponential, then $e^{-x} = 1 / e^x$,
so $f(-x) = 1 / f(x)$. We may not know what the value of
$e^2.4$ is, but we do know it's $1 / e^{-2.4}$.

Here's another example. Given a mortality rate, $m$, the
mean age of death over an age interval, $n$, is
\begin{equation}
    a = \frac{n - n(1+m n)e^{-m n}}{1 - e^{-m n}}.
\end{equation}
If we notice that this equation can be written
\begin{equation}
    a = f(m, n) = n  f_1(m  n),
\end{equation}
then we can design an interesting test.
\begin{eqnarray}
    \frac{a}{n} &= &f_1(m  n) \\
    \frac{a}{n} &= & f_1((2  m)  (n / 2)) \\
    \frac{a/2}{n/2} &= & f_1((2  m)  (n / 2)) \\
    a /2 &= & (n/2) f_1((2  m)  (n / 2)) \\
    a &= &2 * f(2m, n/2) \\
    f(m, n) & =& 2 * f(2m, n/2)
\end{eqnarray}
This result isn't true for almost all functions. It's a strong test
of the implementation. It's an example of the more general
form of symmetry testing, which insists that some transformation
of the inputs has a relationship with some transformation of the 
outputs,
\begin{equation}
    f(g(x)) = h(y).
\end{equation}

\begin{lstlisting}[language=R]
mean_age_of_death <- function(m, n) {
  (n - n * (1 + m * n) * exp(-m * n)) / (1 - exp(-m * n))
}
test_that("dividing by a constant multiplies by a constant", {
  m1 <- 0.01
  n1 <- 1.0
  a1 <- mean_age_of_death(m1, n1)
  for (k in seq(0.1, 5, 0.3)) {
    a2 <- mean_age_of_death(k * m1, n1 / k)
    expect_lt(abs(2 * a2 - a1), 1e-7)
  }
})
\end{lstlisting}

There is a paper that shows symmetry testing catches a lot of bugs.

What do I test when I test such a small equation?
a) typing. It's not small enough not to make a mistake, if you're like me.
b) math of floating point. Small m or n leads to numerical problems.
You decide whether you care about small m.


\begin{callout}
\textbf{Mathematical Complexity}---Has your code editor ever told you,
"Computational complexity 16, project limit is 15. Please refactor?"
\emph{Computational complexity} looks at every if--then--else and counts the
number of possible paths through a function.
Scientific code has high \emph{mathematical complexity,\/} which is a
count of the number of mathematical operators and function calls in
a function. Would you want your linter to insist on low mathematical complexity in
a function?
\end{callout}


\section{Testing the compiler}\label{sec:test-compiler}

Let's say you have a cluster of computers. They are all Intel machines,
but they come from different years. They could be different architectures
or just different steppings of the CPU, which is the least difference
one can have between two chipsets. You compile code on one machine,
or you install some R package that compiles code underneath. When
you move to another computer on another day, that same code fails
to run with the message, ``invalid opcode.''

An opcode is a machine-language instruction. The computer you used
yesterday knew a command that's slightly too fancy for the computer
you're using today, and it failed. You have to clean out the code
and recompile today. It's not that the new computer is just older.
It's still possible that, were you to switch back, there might be
the same message again. The history of CPU opcodes isn't a forward
march into a larger, better set.

That scenario would be annoying, and it's a headache for people
saddled with heterogenous clusters. They solve it with, for instance,
creating architecture-specific shared drives so that they can
install a different version of the software for each version of
system architecture on the cluster. That scenario is, however, a
best case scenario.

The worse scenario, and I wish this were rarer, is that the
software runs on the other computer but gives the wrong error.
It can fail silently. This seems like an abrogation of a compiler's
contract with its faithful coder, but it isn't. That compiler
ensured that the code ran well enough on one target architecture.
The problem is that compiler optimizations at level O3, and above,
are willing to make slightly riskier choices that yield great
performance benefits. These choices are tuned to the architecture
at hand, or as requested by compiler options, and, while they may
run on similar architectures, carry more risk of error on them.
It solves the problem of having languages
that don't clearly represent time-saving invariants and of coders
who can't afford motorcycles.

There's a solution for this, too. Turn all optimizations down
to O2 or lower. Or claim that the CPU uses the SSE2 instruction set.
Unless you're intentionally commemorating the death of Phil Katz, you'll see
this as a severe loss of performance.

The last option to deal with this problem is called acceptance
testing. It means that you create a test to run before you
run code on a new machine. It won't do the work of creating
a separate version of the code, if needed, but it tells you if
you need it. You don't generally care about integer performance
here. You care about vectorized code and floating-point code.
That's where the problems will be, so you can subset the test
suite for these acceptance tests.

Designating a subset of all tests as acceptance tests is an
example of test case prioritization~\cite{rothermel1999test}.
We prioritize tests by hand, by marking tests to be run under
different conditions, but tests for scientific code can be lengthy,
which makes it worth considering automatically reordering tests
so that those that are most sensitive to correctness run by 
themselves, or run first~\cite{yoo2012regression}.


\section{Outside the code}\label{sec:outside-code}

\subsection{Documentation defines what a function wants to be}

The section above, on parameter testing, uses a function that's a hybrid
between Wilson's score interval and a Poisson estimator. That hybrid
doesn't have a name, and there's no paper or Wikipedia entry for it.
We can test that its output is a continuous function of its input
parameters. We can test that it matches known values for limiting cases,
but the right version of this function is the one that a team agrees
is right.

That team is usually not just the people in the room. It includes stakeholders
outside the room. It includes reviewers. It includes future versions of the team.

The tests can be a clear statement of the contract this function upholds.
So, too, can project documents, as mentioned in the data movement section,
where CRUD defines selection from a dataframe. Documentation isn't just
another link in the chain of understanding. It is where the software
developer, the one who wrote the function, states their understanding
of the contract that function fulfills. It's the closest statement,
in English, of what unit tests should assert.

Like all other work, documentation takes time and is prioritized by
how it addresses risk. Give the highest-level explanation first.
This could be a reference to a paper, a book section, documentation from another library.
Then say \emph{why} this function exists and \emph{why} it works the
way it does.

And on the technological side, equations in documentation communicate
clearly with research-oriented readers. Similarly, code examples, run
as unit tests themselves, within the documentation, ensure always-working
examples for development.


\subsection{Team is critical to tests}

Take a function under test and project it onto a
screen at the front of a meeting room. Place around that screen
the team you have or the team you would like to have.
Engineering studies show that the future utility, or burden,
of that function depends no more on its testing than it
depends on the culture of that team~\citep{neumann2016,kanewala2014}.

Culture is beyond the scope of this document, but conversations
aren't. One point to raise in that meeting room could
be the effect of discretization or floating point representation
on the mathematical representation. Another could be a
description of statistical properties of the function, a set
of equations, beyond those to implement the function, that
describe its expected behavior. Someone could raise an observation
that brute force solution is suprisingly possible.
The team could discuss likely sets of parameters that might
matter, some determined by the math, some by logic in the
code. A product manager could supply CRUD requirements.
A systems engineer could describe the expected architectures
and risk of failure due to optimization levels. Whether these
are separate people or roles for a very few people, their
communication gives testing the best chance at relevance.

Testing well is helpful, but we saw, in the section about
data movement, that identifying what is data movement
and what is an algorithm meant we could label the two.
Giving a function a good name is important to a software
developer, but scientific code is rarely owned by a team
of software developers. It's shared by a team of researchers.
That team has a body of knowledge about the software, and,
the same as any body of knowledge, it's a shared set of
understanding where new things are added and less useful
ideas are dropped.

In this research environment, collaboration on testing
creates commonly-understood, trusted functions. These, more
than any single snapshot of the whole calculation, are
what the group understands about the code. These trusted
functions will be copied into other projects, translated
to other languages. They will exist long beyond the work
that has your name.

\section{Further Reading}\label{sec:further-reading}
\begin{itemize}
	\item \emph{Software Testing: A Craftsman's Approach,}~\cite{jorgensen2013}.
	\item \emph{Software Testing and Analysis: Process, Principles, and Techniques,}~\cite{pezze2008}.
	\item \emph{Working Effectively with Legacy Code,}~\cite{feathers2004working}.
	\item \emph{Software Engineering for Science,}~\cite{carver2017}.
	\item ``Testing Scientific Software: A Systematic Literature Review,''~\cite{kanewala2014}.
	\item ``Development of Scientific Software and Practices for Software Development: A Systematic Literature Review,''~\cite{neumann2016}.
	\end{itemize}

\appendix

\section{Faults in Mathematical Code}\label{sec:faults-and-failures}

For each section, ask \aside{what faults does this technique limit?}
For instance, using a limit test for constant-mortality mean age
ensures you don't have a numerical fault.

\subsection{Fault versus Failure}
Jorgensen's \emph{Software Testing: A Craftsman's Approach} distinguishes
faults from failures~\citep{jorgensen2013}.
A fault is the characters that were typed
incorrectly. A failure is observation that something went wrong.
For instance, there may be a fault that 0.3 is assigned to an integer
instead of assigning it to a double. The failure will happen later,
in the code that divides $1 / 0$.

Any failure is a problem. We don't want to throw exceptions or segfault.
Worse for mathematical code is when an error in the code isn't visible
but does give a wrong answer.
The worst problem is faults that aren't failures.


\subsection{Classes of Faults}
Jorgensen gives a set of fault categories and examples, elided here:
\begin{itemize}
    \item Input/Output - incorrect input accepted, incomplete result
    \item Logic - missing cases, test of wrong variable
    \item Computation - incorrect algorithm, parenthesis error
    \item Interface - parameter mismatch, call to wrong procedure
    \item Data - Incorrect type, wrong data, off by one
\end{itemize}
These all apply and, yes, read Jorgensen, but most of this article 
expands upon a single category of fault: incorrect algorithm.

This document covers two different kinds of failure-free faults.
\begin{itemize}
\item Simple data manipulation that doesn't do what the user
   expected it to do.
   
\item Mathematical functions that return a result that looks
   reasonable but isn't correct.
\end{itemize}
While these faults are within the taxonomy of faults
in any book on testing, we can discuss some of the particular
challenges that complicated mathematical functions introduce.

\bibliography{tsci}
\end{document}
