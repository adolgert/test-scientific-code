%!TEX program = xelatex
\documentclass[fleqn,10pt]{olplainarticle}
% Use option lineno for line numbers 
\usepackage{xcolor}
\usepackage{framed}
\usepackage{hyperref}
\colorlet{darkblue}{blue!40!black}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    citecolor=darkblue
}
\urlstyle{same}

\usepackage{xspace}
\usepackage[utf8]{inputenc}
\usepackage{fontspec}
% This section makes Fira work.
\setmonofont{Fira Code}[
  Contextuals=Alternate  % Activate the calt feature
]

\makeatletter
\def\verbatim@nolig@list{}%
\makeatother

\usepackage{listings}
\usepackage{lstfiracode}
\usepackage{url}


% For natbib, \citet for textual, \citep for parenthetical,
% \citeauthor and \citeyear.
\newenvironment{callout}
{
\begin{figure}
\begin{center}
\begin{minipage}{0.9\textwidth}
\begin{framed}
}
{
\end{framed}
\end{minipage}
\end{center}
\end{figure}
}
\newcommand{\rlang}{\textsc{r}\xspace}
\newcommand{\cpp}{\textsc{c}++\xspace}
\newcommand{\cpu}{\textsc{cpu}\xspace}
\newcommand{\nan}{\textsc{NaN}\xspace}
\newcommand{\ieee}{\textsc{ieee}\xspace}
\newcommand{\aside}[1]{\textcolor{red}{#1}}

\title{Testing Scientific Code}

\author[1]{Andrew Dolgert}
\affil[1]{IHME, University of Washington}

\keywords{software testing, unit testing, scientific software}

\begin{document}
\lstset{
  style=FiraCodeStyle,
  basicstyle=\footnotesize\ttfamily,
  %backgroundcolor=\color{green!20}
  }

\begin{abstract}
When we write code to do science, even though we have purposefully
chosen each step of the code, we may not know what the exact result should be.
Not having an exact right answer makes unit testing both complicated and
critical to trusting the scientific results. This article doesn't show how
to set up unit testing in \rlang, Python, or Julia. It doesn't cover numerical
analysis. It shows how to put boundaries on the behavior of mathematical
code. Some techniques are adaptations of traditional unit testing methods
and others are particular to testing scientific code.
\end{abstract}

% Audience:
% a) researchers who write code but don't know how to turn notebooks
%    into unit tests.
% b) software engineers who implement code but aren't sure how to handle
%    tests of squirrelly math.

% Intention:
% Sleuthing to reveal how mathematical expectations become test assertions.
% Literature says research code has an "oracle problem."
% I don't like that term. So what if we don't know the exact number
% a function can produce? All unit testing, mathematical or otherwise
% is about testing whether expectations are met, and those can
% be hard to define. We always have some expectations
% about how new research code will behave. The challenge is to
% a) understand what our expectations are and b) express those
% expectations in unit tests in c) a way that reduces risk.

% Major sections
% 1. Reading for risk
% 2. Parallel implementation
% 3. Statistics for tests
% 4. Deterministic repeated measures
% 5. Methods and motives, the process in context

% finite state machine structured testing

% From Mike Richards
% 2. Focus on moving up and down the stack, not between scientist and developer.
% You don't need to tell them everything about the subject.
% Alec contributed Pearson
% James presses about the audience and software vs. math.

% TODO
% Missing: fault localization, model-based testing
% add simple stats tests from the intro modern stats book
% add figures to explain pearson coefficient
% explain mantissa and exponent.
% add more symmetry examples.
% show path analysis and flow analysis as figures.
% Take a failing test and reduce the input until you have
% a smallest failing test. Very helpful for identifying failures.

% High level tests will test more.
%                  will make it harder to find a fault.
%                  will make it harder to identify a _failure._


\flushbottom
\maketitle
\thispagestyle{empty}

\tableofcontents
\section{Introduction}\label{sec:introduction}

% In most code, you expect a value. For scientific code, your
% expectations are properties of the code or in relation to
% previous work. It's about discovering expectations.

The code for a scientific application, laid out across a monitor,
will have plenty of functions that look like those of any other application.
It will parse command-line flags, parse input files, and maybe even have
an event loop. Some portion, however, will look entirely different. It
may be laden with long equations, or it may be integer manipulation or
graphs, but the functions that do this work are complex, and they carry risk.

The complexity of those scientific functions is necessary
in order to model some feature of the world. Their math, whether mundane
or arcane, makes it difficult to use traditional unit tests to address
the risk of failure in these functions. When a model is supposed to represent
a subtle behavior of the world, testing that its output is correct feels
like an existential assertion.

However, scientific models, and the functions that comprise them,
always have behavior we can discover, characterize, and assert in a unit
test. Some of that behavior is approximate and some exact.
It's this mathematical behavior that is our first concern for testing.
It's called functional testing, when we look for faults in the code
that would lead to failures when the code runs. We exclude, here,
questions of testing how long a function runs, how much memory it uses,
whether it's secure, or whether it's usable.

I met a man, at dinner at a wedding, who told me he quit his
job as \textsc{ceo} of a printer company in order to lead
the support division. He said the company was going downhill,
and there was no better way to find out why than to hear from
the customer about how their product failed.
For scientific code that supports research, it can be hard to
know when it's right or wrong, and unit testing is the
support division.

Even with this scope, of whether the function will, in the future,
give the right answer, it will become clear that, as complex as
code looks on a monitor, the work to reassure ourselves that it lacks
faults, that there is no character out of place, would be terribly more
work than to write the function. If each part of a scientific application
had an exhaustive suite of tests, it would become brittle to change,
the opposite of a tool for research.

The code we test has a topography of risk. Some of it is measurable,
as a number of if--then statements or a count of equation terms.
Some of it is a landscape of doubt, that these lines of code are
doing their job or that that language idiom is properly-written.
When a function fails to read a file, and it quits, that's less
a problem than a function that reads a file incorrectly and gives
a wrong result.

Assuming we've read a good unit testing book like \cite{jorgensen2013},
and that we can figure out, elsewhere, the idiosyncracies of a
particular language or testing framework,
let's assess here the risks that are unigue to scientific code.
Let's look at how the tools we already know can address those risks
and how some new tools might help when the stakes are high.



\section{When a fault is a problem}

Debugging code can take longer than writing it. You can see
a wrong answer and spend hours trying to figure out where the
calculation went wrong, in what function, in what data structure,
at what assignment statement.
Writing unit tests is like debugging code before you know
where the bugs are.

We prepare to test code by walking backwards, starting with
what we consider a problem. For this work, we want to find
ways the code will stop prematurely (or never stop)
and ways it will give a wrong answer masquerading as a right one.

Failures are evidence something went wrong. Since our main goal
is scientific correctness, we classify failures on two axes,
whether they are recoverable and how long it takes to discover
the failure. Some algorithms are allowed to fail for certain
input values, and we can recover by running a different, acceptable
algorithm. Failures can be immediate exceptions that provide
debugging information or abnormal termination that quits with
only a single error code, but they
could be \nan{}s that silently propagate through code. They
could be loops that don't terminate, so you wait hours to find
the problem.

How your scientific application handles failures, and how you
rank their severity, varies by application, as we can see from
a few examples. If a software application has a menu of operations
to perform, and one menu item incurs a fault, which creates an exception,
then maybe the user sees an error message. That's not intended, but
it means this one operation won't work on this data.
If the scientific application is part of a large pipeline of
work, and a fault creates a result that is out-of-bounds because
it is negative or \textsc{NaN}, then that could be \textsc{ok}
if there is a backup calculation that is more likely to succeed.
In most cases, if I'm doing science, my main worry is when a
fault \emph{doesn't lead to failure.} It's numbers that are
significantly different from the correct value but not obviously
wrong. It's faults without failures.

Failures come from defects in the code, which we call \emph{faults,}
and we classify these so that we know which are most important
to our task. The ODC classifies types of defects, in order to understand
how different types affect code across its development time.
\begin{itemize}
    \item \emph{Algorithm}---These are correctness problems
    that can be fixed within a function or local data structure.
    This includes both whether an equation is correct and whether
    a loop has an off-by-one error.

    \item \emph{Assignment}---These are incorrect initialization
    of control blocks or data structures.

    \item \emph{Checking}---Program logic around loop conditions
    and branching statements. This is about a wrong if--then. A mistaken condition can mean that
    the result would be correct for a different set of parameters.

    \item \emph{Interface}---Defects when calling other components
    or functions. This includes parameter lists.

    \item \emph{Functional}---This doesn't mean that defects are
    in functions. It means these are errors that require redesign
    of the code. It can mean moving code among functions, changing
    important data structures, or redesigning objects.

    \item \emph{Documentation}---Whether it's published documentation,
    code documentation, or maintenance notes, there can be
    correctness errors here.
\end{itemize}
Scientific equations are in the algorithm category, so that category
is the most immediate threat to correctness, but the
ODC paper observed that teams find algorithm defects early in the
development process. Meanwhile, faults in assignment and checking were
found later in the development process. A fault in checking would
mean an incorrect branch in the code. Even if the algorithm is
correct, the answer from the code would be inappropriate, so these
are no less wrong.

I read a story in the observations of the ODC paper. It says
that we first focus on the core math, and it has errors, but we
gradually work those out. Meanwhile, we build software around that
core math, and that software has features and options that multiply
algorithmic complexity and thus multiply problems.

Between these two kinds of errors, the algorithmic versus the
checking and assignment, there is difference in how they fail.
If an equation has a misplaced parenthesis or incorrect exponent,
it is more likely to return a number without raising an exception.
On the other hand, an off-by-one error in a loop, when it fails,
is more likely to create an out-of-bounds memory error. This visible
failure is a real problem in production code, but it is far
less insidious than a silent fault in the equation.

There's another classification scheme for faults that asks
where they happen, in what part of the code. For scientific computing,
we can use a traditional model of high-performance computing
to break the \emph{where} into a few pieces.
\begin{itemize}
    \item \emph{Parameter handling}---Faults interpreting input parameters.
    There is often an early step to turn the parameters a user specifies
    into an internal form that the algorithms need.

    \item \emph{Input/Output}---Faults when reading and writing
    data from/to any other component.

    \item \emph{Code movement}---The work to move data from input
    to the right data structure for an optimized algorithm. The work
    to take data out of one data structure and put it into another one.

    \item \emph{Algorithm}---Faults in what we consider the main
    computation. We tend to write code that centers on one
    or more algorithms.

    \item \emph{Visualization}---This is any output for the user. It
    can be in a window or graphs on disk. We separate this from the
    data input and output because it often has its own set of
    algorithms which can be as complicated as any other part of the code.
\end{itemize}
These different parts of the code tend to have different maturity levels
and different kinds of risks. They develop differently over time.
For instance, visualization algorithms can come pre-packaged, so that
most of the work is setting up the data and checking that it is valid
for the visualization method. For visualization, the failure may look
like a divide-by-zero, but the fault is more often input data that
is in error.

Code movement tends to have checking- and assignment-type errors,
while algorithm code tends to have, well, algorithm-type errors.
This points to a principle for how to group code.
\emph{Code Complete} gives seven types of cohesion that encourage
grouping code together~\citep{mcconnell2004code}. We can add to that list. We should group
code that needs similar kinds of unit testing because it poses
similar kinds of risk. Here, that means we would do complete
path testing on parameter handling. We might test random samples
of data structures for code movement portions.

We use unit tests to look for faults, but unit tests observe
failures. We can test less, and test better, by understanding
how to read code so that we see cause and effect.


\section{Path and data flow}

My first assignment in high school chemistry was a six-inch square
black box on my desk. Inside the box was some cardboard structure
and a single marble. I had to make a sketch of the hidden cardboard
structure by turning the box and feeling the weight of the marble
and as it moved. Even if I couldn't see inside the box, I
understood how a marble would balance, roll, and drop.

A unit test sets parameters on a function and checks its result.
I might get to read the code I'm testing (clear-box testing),
or not (black-box testing). In either case, I need a feel for
the physics of how each probing parameter might affect the
function's return value. Analyzing cause and effect in code
helps us read it. There are only three was we find problems
in code. We read it during code reviews, we test it, or we run it on
real world problems. The first two rely on reading code well,
and the third is the definition of failure.

\subsection{Code paths}\label{sec:code-paths}

If I look at the code in a function, it typically has
some if--thens, some while--loops, some for--loops. These
control structures determine which lines of the code do
or don't get executed, depending on the input parameters.
When we analyze code according to its control structures,
it's called path analysis.

Let's take a running average as an example. This code
averages values over several days, and it will take an
average of the last several days, the next several days,
or in-between. It's implemented without using \lstinline|numpy|,
as an example. Look for control structures in this function.

\begin{lstlisting}[language=Python,numbers=left]
def running_average(x, duration, orientation, fill):
    offsets = {
        "past": 1 - duration,
        "future": 0,
        "midpoint": 1 - ((duration + 1) // 2)
        }
    offset_in_window = offsets[orientation]

    result = x[0:len(x)]
    if fill:
        filled = x[:1] * (duration - 1) + x + x[-1:] * (duration - 1)
        offset_in_window += duration - 1
        for i in range(0, len(x)):
            a, b = [i + offset_in_window, i + duration + offset_in_window]
            result[i] = sum(filled[a:b]) / duration
    else:
        for i in range(0, len(x)):
            a, b = [i + offset_in_window, i + duration + offset_in_window]
            left = max(a, 0)
            right = min(b, len(x))
            result[i] = sum(x[left:right]) / (right - left)
    return result
\end{lstlisting}

\noindent{}If \lstinline|fill=True|, the lines 11--15 are run.
If it's false, lines 17--21 are run. We can further subdivide
these two blocks by observing that, if the length of \lstinline|x|
is 0, then the for--loop won't run. Therefore, that's a separate
block too. The blocks above are
\begin{enumerate}
    \item Lines 2--10, Set offsets
    \item Lines 11-13, Create filled array
    \item Lines 13--15, For--loop over filled array
    \item Line 17, Without filling
    \item Lines 17--21, For--loop over array
    \item Line 22,
\end{enumerate}

There are four paths, which depend on \lstinline|fill|
and \lstinline|len(x) > 0|.
\begin{itemize}
    \item 1, 2, 3, 6
    \item 1, 2, 6
    \item 1, 4, 5, 6
    \item 1, 4, 6
\end{itemize}
Two of them are much less likely. You can either decide that
they are less important because they are less likely, or
you can decide that they are important to test separately
because other parts of the tests will cover the more common
paths.

Not every if--then is a separate path. Consider a case where
the same option is used twice.

\begin{lstlisting}[language=Python,numbers=left]
def sample(x, option = false):
    if option:
        k = 3
    else:
        k = 2
    y = k * x
    if option:
        z = [abs[m] for m in y]
    else:
        z = y
    return z
\end{lstlisting}

\noindent{}There are two if--thens, which would lead to four different
paths, except that they use the same condition. Depending on
how complicated the condition is, it may not be obvious that
the paths are related to each other.

Path testing is an approximation to the number of different
ways to call a function. Looking at the code line-by-line,
we can see finer branches in how a function handles inputs.
Branch points are conditions that depend on input values
but don't change which lines of code are run. For example,
data frames are built to enable easy branch points.
\begin{lstlisting}
under30 <- individuals_df[individuals_df$age < 30, ]
\end{lstlisting}
It isn't an if--then, but data above and below 30 will take
different paths through the code. Data frames are built
to handle complicated inputs, so they are built to create
complicated testing problems.

Path testing is a way to get a clear view of
the code, but it's just one model for how to read code.

\subsection{Data flow}
Let's look at another piece of code, this one to integrate
a differential equation.
\begin{lstlisting}[numbers=left]
function explicit_euler_riemann(mu, beta, p0, h, t0, t1, a0, a1)
    a = a0:h:a1
    t = t0:h:t1
    m = length(a)
    n = length(t)

    u = zeros(h, m, n)
    u[:, 1] = p0(a, t0)

    for i in 1:(n - 1)
        u[:, i + 1] = (1 .- h .* mu(a, t[i])) .* u[:, i]
        u[1, i + 1] = h * sum(beta(a, t[i]) .* u[1:m, i])
    end
    u
end
\end{lstlisting}

Pick an important assignment in the function, say line~11.
Then trace backwards and forwards to see what affects this line
and what this line affects. We can say \lstinline|u| depends
on \lstinline|h|, \lstinline|a|, \lstinline|t|, and \lstinline|u|.
We can trace back to line~8 that \lstinline|u| depends on
\lstinline|a| and \lstinline|t0|. This dependency chain is a
graph of cause-and-effect through the data in the function.

For the code above, we could be more precise if
we treat each column of \lstinline|u|
as a separate piece of data. We say \lstinline|u[:, i+1]|
depends on \lstinline|u[:, i]|, all the way back to 
\lstinline|u[:, 1]| which depends on \lstinline|a| and
\lstinline|t0|.

If the output value, \lstinline|u|, has an error in row $k$,
\lstinline|u[k, :]|, then we know from the data flow that
this row doesn't interact with any other rows, until possibly
setting the initial column, \lstinline|u[:, 1] = p0(a, t0)|.
We see that an error in any row should propagate, on line~12,
to the first row. If it doesn't, then it must be balanced out
somehow. We can begin to reason about where the fault must
be that caused the observed failure.


\subsection{Complexity and coverage}

Sometimes your code editor will read your code in order to help
you assess risk. Most editors have add-ins that will compute
computational complexity, even as you type. Cyclomatic complexity
is a count of the number of code paths, as described in Sec.~\ref{sec:code-paths}. A script without functions can have a computational
complexity in the thousands. Most small functions have a computational
complexity under sixteen, meaning there are less than four binary
decisions (2 x 2 x 2 x 2). High computational complexity is a
classic indicator of risk.

Scientific code has high mathematical complexity, which is a count
of the number of mathematical operators and function calls in a
function. We know \lstinline|2*x| is less complex than
\lstinline|a*x^2 + b*x + c|.
I haven't seen a general tool that calculates this. Some tools
calculate a related set of metrics called Halstead complexity,
but those count every operator and operand in a function.
When I estimate risk in a mathematical function, I look at
the number of equations and the number of operators and operands
in those equations in order to gauge how likely I misplaced a
parenthesis or juggled two variables.

Unit testing tools often report coverage metrics. These metrics
estimate how much of your code is tested. The most common metric
estimates what percentage of executable lines of code were invoked
by any unit test. It's equivalent to reading the code for paths.

Counting code paths undercounts the possible behaviors of the code.
Every decision-point in the code generates a new path, so it's an
if--then or a switch--case statement. Different branch statements
don't generate a new code path.

Counting code paths overemphasizes the importance of testing every
line of code. There can be an if--then statement where one branch
is either simple or will fail loudly if it's wrong, so we don't test
that first.
There are sections of code that carry less risk.
While testing reduces risk, it makes changing code more difficult.
Don't take one-hundred percent code coverage as a goal. Some
projects do set a coverage goal at 70\,\% or 90\,\%, which is reasonable.



\section{Testing numbers}\label{sec:ieee-numbers}

If we take a look at a function that is nothing but a simple equation,
\begin{lstlisting}
quadratic_root_positive <- function(a, b, c) (-b + sqrt(b^2 - 4 * a * c)) / (2 * a)
\end{lstlisting}
it looks at once familiar to a mathematician and to a software engineer.
It's clear to read, succinct in computation, and a well-known mistake.
The variables \lstinline|(a, b, c)| aren't real numbers. They are floating-point
numbers. While most software uses floating-point (floats) to tell time
or count, these floats are hard-pressed by the more difficult task of doing
math. Math with floating-point is a domain-specific skill that is new to
most practitioners of scientific coding, so it's a source of confusion and risk,
both for writing code and for testing it.


\subsection{Floating-point data type}

A floating-point data type is usually fine taking the place of a real number
in an equation. It often gives results that are correct to a decimal place
of $10^{-17}.$ All of the common operations on real numbers are quick with
the floating-point data type. It's a miracle in most cases.

The \ieee~754--2008 standard leads the \cpu, the operating system, and 
the language runtime, together, to guarantee a consistent, efficient environment
for code to run. This minor miracle is fascinating~\citep{muller2018handbook},
but its success is that you don't need to know more detail about it than
you might find in \emph{Numerical computing with IEEE floating point arithmetic}~\citep{overton2001numerical,goldberg1991every}.

The data type we store in programs is usually encoded in a binary representation
of four or eight bytes, which are 32 or 64 bits. Those bits represent three integers,
one for the sign, one for the exponent and one for the significand. The sign is
a single bit for $\pm 1$. The exponent is a power, the same way we would write
scientific notation, except here the power is $2^e$, where $e$ runs from a
negative value to a positive one. The significand is a series of bits that
usually represent a number between one and two, $1 \le s < 2$, except in the
case all bits are zero, which represents zero. None of these three
integers are continuous values, but they can represent values that are
sufficiently-close to every real value between some large negative number
to some large positive number.

Floating-point types represent more than just real numbers, though. They
also take the value $-\infty$ and $\infty$. In order to represent the inverse
of these values, floating-point encodes both $0$ and $-0$, so that $1/(1 / -\infty)=-\infty$.
Lastly, a floating-point type can indicate it isn't a number at all,
by representing \nan.

When floating-point calculations are working correctly, they can
be unintuitive.

\begin{lstlisting}
0 == -0  # True
-0 < 0  # False
NaN == NaN  # False
NaN < NaN  # False
is.nan(3 * NaN)  # True
\end{lstlisting}

There seems to be a trick to asking whether a value is
or isn't a \nan. Every language supplies some form of \lstinline|is.nan(x)|,
and it's the only good way to find a \nan. There isn't even a single,
defined bit pattern that represents \nan values, and library implementers
have been known to sneak a birth date in there.

The \nan value is designed to fail silently but visibly. It plays
a crucial role in how floating-point is supposed to fail gracefully.


\subsection{Floating point exceptions}

In those cases where a mathematical operation would be undefined
for a real number, or where a floating-point type can't represent
the answer, the \ieee~754 standard suggests using \nan or $\infty$ in order
to alert the process that there has been a problem.

There are three types of \emph{common floating-point exceptions}.
Here, an exception isn't a \cpp{}-kind of exception. It means that
there was a reported failure in a common operation.

\begin{tabular}{lll}
Invalid Operation & & \\
 & \lstinline|1 / 0| & \nan \\
 & \lstinline|0 * Inf| & \\
 & \lstinline|Inf / Inf| & \\
 & \lstinline|1 REM 0| & \\
 & \lstinline|sqrt(-1)| & \\
 & \lstinline|NaN < NaN| & \\
 & invalid conversion & \\
Division by Zero & & \\
 & \lstinline|1 / 0| & $\pm\infty$ \\
 & \lstinline|1 * log(0)| & \\
Overflow & & \\
 & \lstinline|exp(710)| & $\pm\infty$ or largest value\\
\end{tabular}

When we say that an overflow can result in either $\infty$ or
the largest-representable floating-point value, that means you can
usually choose how your code will handle the situation. You usually
make these settings by telling the language runtime. That means your
code will make a function call that requests that overflow use the
largest value instead of infinity.

More important for debugging, Windows, MacOS, and Linux, all give
a process the option not to fail silently. In some cases, that means
the operating system will kill the code when there is a floating-point
exception. In some cases, the code can request that a handler-function
be called. These are tools to help you find the fault that caused the
failure. If you see a \nan and want to know when it happened, you can,
in most languages, ask that the code stop the moment it would have made
the \nan. Then a debugger can find the fault.

A downside to making floating-point behavior configurable is that
it might not be consistent from installation to installation.
If this behavior is critical, and there are multiple target runtime
environments, it can be helpful to unit test the necessary behavior.
For instance, if a Python Pandas data frame returns a \nan when it
takes a square root of a negative number, and my code relies that behavior,
then I make a test that runs negative data through the code.
\begin{lstlisting}[language=Python]
def test_sqrt_negative_is_nan():
    df = pd.DataFrame({"a": [3.0, -3.0]})
    df.b = np.sqrt(df.a)
    assert(np.isnan(df.b[1]))
\end{lstlisting}
The risk is that some part of the software stack, from Pandas to Python, from
Windows to someone else's installation of Windows,
will make some choice that changes the behavior that I see today.


\subsection{When are floats equal?}

Comparison of two floating points, the regular kind that aren't
infinite or \nan, has its own problem. If you set
\lstinline!x = 3.0! and \lstinline!y=9.0 / 3!, then you will probably find they
aren't equal. At least, \lstinline!x==y! will fail most times, because
floating point math is usually approximate. We test for this
using a value called \emph{machine epsilon.} Every language defines
this value. It is the least difference between the
mantissa of two numbers.

Every floating-point number is expressed as a mantissa times
an exponent. It's always of the form $1.m \times 2^e$, where
$m$ is a bunch of digits after the 2 and $e$ is the exponent
for the power of 2. There's a bit for the sign, too.
Machine epsilon is the smallest difference in the mantissa,
excluding the exponent, so if you want to test that two
floating point numbers are equal, you test relative error using
machine epsilon.
\begin{lstlisting}
n <- 4
same <- x * (1 + n * machine_epsilon) > y && x * (1 - n * machine_epsilon) < y
\end{lstlisting}
That value of $n$ is a slop factor, but there is some sense
to it. Each time a function does another floating-point calculation,
it can introduce more drift. If you want to know whether
$3.0 = 9.0 / 3$, then you can use $n=1$, but if you want to know
whether a sequence of a thousand multiplications yields the
same result, then use a larger $n$.

For single-precision floating-point, which are 32~bits in size,
or four bytes, epsilon is near $10^{-7}$. For double-precision
floating-point, which are 64-bits in size, epsilon is near
$10^{-16}$.

In practice, when we test to see if a number is what we expect,
we're testing whether an algorithm has given the correct result.
That means there were many operations, with many rounding errors,
leading to that number. As a rule of thumb, each floating-point
operation, which is an addition, subtraction, multiplication,
or division, will add another few epsilon to the relative error.

If you're computing with doubles, and the result is off by $10^{-4}$,
is that big or small? If this is a single function you're testing,
that's probably a huge error. If it's the end of a long calculation,
or if it's the kind of calculation that divides by a small number somewhere,
then it could be fine.


\subsection{Discovering numerical analysis}

At the start of Sec.~\ref{sec:ieee-numbers}, I said that the
quadratic formula had a problem. For positive values of $b$
and small values of $a$ and $c$, the quadratic formula subtracts
a number from nearly-the-same value. We know that floating-point
types are excellent at representing numbers near zero, but they
lose precision for larger numbers. By playing with the quadratic
equation, we can see a way to write it that doesn't have the same
problem.

\begin{lstlisting}
quadratic_root_positive <- function(a, b, c) (-b + sqrt(b^2 - 4 * a * c)) / (2 * a)
quadratic_root_positive_alt <- function(a, b, c) 2*c / (-b-sqrt(b^2-4ac))
\end{lstlisting}

If we use the second version whenever $b>0$, then there won't
be difficulty.
There is a similar rewriting for $b<0$~\citep{overton2001numerical}.

If I had written the first equation and wasn't working through
numerical analysis of it, how would I know there was a problem?

\begin{lstlisting}
test_that("quadratic formula OK for small a and c", {
    a <- 1e-2
    b <- 50
    c <- 1e-5
    root <- quadratic_root_positive(a, b, c)
    agreement <- a * root^2 + b * root + c
    expect_true(abs(agreement) < 1e-10)
})
\end{lstlisting}

The quadratic equation, in the code, was never wrong. It returned
the correct values for the equation, given that its domain is
three floating-point types. The quadratic equation in the code
is also the one you have in hand. The unit test asks whether
it meets the expectations we have of the quadratic equation
acting on real numbers. We assess our expectations and judge
where they might not be met.

\noindent{}The unit test asks whether the root does produce a
zero value. It's careful about estimating how precise that value
should be. The value $10^{-10}$ is smaller than expected error
for single-precision floats but should be well within the realm
of error for double-precision floats. We would guess to test
in this region because we know that, for floating-point types,
it can be important to test near boundary values, including
zero, because the data type has different behavior.

There are many cases where the representation of floating point
creates problems for math in code. Take, for example, calculation
of the mean age of death, for a model that assumes constant
mortality across an interval.
\begin{equation}
  a_x^{cm} = \frac{1}{m_x} - \frac{n_x e^{-m_x n_x}}{1-e^{-m_x n_x}}
\end{equation}
A straightforward implementation in Julia looks close to the equation,
except that this language uses periods to denote elementwise
operations.
\begin{lstlisting}
function constant_mortality_mean_age(mx, nx)
    expx = exp.(-mx .* nx)
    (1 ./ mx) .- (nx .* expx) ./ (1 .- expx)
end
\end{lstlisting}
There is a problem as $m_x\rightarrow 0$. This equation should
approach $a_x=0.5,$ but it loses precision as the denominator
nears zero.

If we hadn't looked at the equation in detail, but we had implemented
tests that probe the range of possible input values, then
we would see the numerical problem arise.
\begin{lstlisting}
mx = vcat([0.9 * 10.0^i for i in 0:-1:-17], [0.0])
nx = ones(size(mx))
ax = constant_mortality_mean_age(mx, nx)
println(ax)
for check in 2:length(ax)
    @assert ax[check] > ax[check - 1]
end
@assert all(ax .< .5)
\end{lstlisting}
This test asserts that the values get closer to 0.5 from below.
Not only will the value at 0.0 yield a \nan, but also
values before then return in the thousands or \textsc{Inf}.
Once you see the problem in the unit test, it's possible
to rearrange the equation to avoid
division by zero. In this case, the distance from the limiting
value of $n_x/2$ is the Langevin function, $L(x) = \mbox{coth}(x) - 1/x$,
which has well-known limiting forms near zero.
\begin{lstlisting}
function constant_mortality_mean_age(mx, nx)
    (nx / 2) .* (1 .- langevin_function.(mx .* nx / 2))
end
\end{lstlisting}
This equation is stable near zero mortality rate.
As with the quadratic equation, a little rewriting of the
algebra creates a more stable solution.

The unit test won't tell you how to fix the problem.
It can show places where more numerical analysis is
needed in order to have a stable representation of the math.
In the same way that a floating-point type mirrors important
properties of real numbers, we can modify a function to
mirror important properties of the math we want it to represent,
even though it will always remain a fundamentally different function.


\section{Choosing parameter values to test}\label{sec:parameter-logic}

\subsection{Decision tables}\label{sec:parameter-decision}

A unit test is all about what parameters go in and what result
comes out. There is setup of variables and
parameters, which can be complicated. Checking the result can
require twists and turns, but unit tests reduce to parameters and
result.

Let's say, for our scientific code, that we can figure out what the result
should be, for any set of input parameters. This isn't always
possible, which is a problem we can discuss later, but assume
for now that we have a way to know what the answer should be.

Decision tables capture the clarity of what-goes-in paired
with what-comes-out. They have one row per test with one
column per input or output. It sounds simple, but it
really makes code clear to use exactly that, a formatted
table in a unit test.

For instance, consider a function that projects a time series
using the trend of its running average. It could use the slope
of the last $m$ entries in the time series.
\begin{lstlisting}
def trend_predict(x, m, kernel, transform, range)
\end{lstlisting}
It could weight
those last $m$ entries according to an exponential or Gaussian, so that
earlier entries count less. It could transform the entries
into log-space before projecting them. These options
($m$, kernel, transform, and range) create
sets of allowed parameters.
\begin{center}
\begin{tabular}{|l|l|l|l|l|l|}\hline
$x$ & $m$ & kernel & transform & range & output \\ \hline
$x_0$ & 5 & Gaussian & linear & 1.0 & 2.7 \\
$x_0$ & 5 & Gaussian & logarithmic & 2.0 & 1.9 \\
$x_0$ & 5 & exponential & linear & 3.0 & 2.8 \\
$x_0$ & 5 & exponential & logarithmic & 2.0 & 2.1 \\ \hline
\end{tabular}
\end{center}
We can see that, even for
this small case, the list will be long. The example
above doesn't even begin to probe error conditions, such
as negative $m$ values or $m$ greater than the length
of the data. For the time series itself, we could imagine
a lot of time series that might be different in some way,
if they were negative values or all the same value.

We need to decide which parameter sets to test
and how many are enough.

\subsection{Equivalence classes}\label{sec:limits}

\subsubsection{Definition}
% This is called Partition testing
% You split by functionality, specification, requirements,
% or you split by statement coverage, code coverage, branch coverage.
% 
% If everything in a partition fails, it is "homogeneous."

The tools we discuss here will help us with the three steps
we take to choose which parameters to test.

\begin{enumerate}
  \item Define which parameters could break the function in different ways.

  \item Assign a probability to each set of parameters.

  \item Choose an order in which to test some, or all, of those
   parameters.
\end{enumerate}

\noindent{}These three steps will help us focus our testing
where we see the most risk, and they start with identifying
equivalence classes.

Equivalence classes are sets of parameters that test the same
parts of the function. If one of them would find a failure,
then another in the same equivalence class would find the same
failure. That's a reasonable way to partition test parameters
and impossible to do in practice.

We could ensure that each line of code in the function is
run by at least one unit test. That's called statement coverage.
We could further fine-grain that to ensure that every branch
in the code is taken by at least one unit test. That's
branch coverage. Both of these are kinds of code coverage.
They're both incomplete representations of equivalence classes
in code and, in practice, we often think only of statement
coverage.

Take the example above, and list equivalence classes for each parameter.
\begin{center}
\begin{tabular}{|l|l|l|}\hline
parameter & \# classes & classes \\
$x$ & 5 & positive, non-positive, constant, linear, very noisy \\
$m$ & 5 & negative, 0, less than data, equal to data, more than data \\
kernel & 2 & Gaussian, exponential \\
range & 4 & negative, 0, positive, longer than $x$ \\
transform & 2 & linear, logarithmic \\ \hline
\end{tabular}
\end{center}
That makes $5\times 5 \times 2 \times 4\times 2 = 400$ cases.
By defining a limited number of equivalence classes for
each parameter, we have reduce the total number of unit tests
from infinite to 400, but that's still a lot.

Our equivalence classes include edge cases. If a parameter
is a string, edge cases are strings of length one and zero.
If a parameter is an exponent, edge cases are $(-1, 0, 1)$,
if we think the power of zero matters.

When I identify edge cases, I'm assigning a higher probability
to the risk associated with these parameter values. I'm
giving them a separate equivalence class in order to ensure
that there is some coverage of these cases. That was
step two.

When I choose equivalence classes, I can start by reading
the code. Defining equivalence classes by code coverage is
exactly the same as reading code for path dependencies.
I can also start by reading the specification or requirements
document, were those to exist. For scientific code,
it can help to read the math this code represents.
A parameter of 3.0 and 3.5 might not be equivalent if they
represent radians. Floating-point types only make this
more complicated.

\subsubsection{Mathematical parameters}

A function, like the example, has three kinds of parameters.
The vector is data. Then there are constants for the calulation.
Finally, there are options for the algorithm. These accrete
as the function ages. The code sees all of these as parameters,
and each one has equivalence classes that expand the
number of possible tests we could do.
Floating-point types, integers used for math, and vectors
of both of these, all create large numbers of equivalence classes.

If the function-under-test represents a math equation, then
look to any special values for that math equation. Identify
when a denominator can be zero. Look for values that,
if set just right, will cancel each other out, because
floating-point subtraction, $b-a$, can lose precision of
$b$ and $a$ are far from zero. It's more rare, with double-precision
floating-point types, to worry about values that are too
large or small.

An integer type can have similar trouble representing
the integers. The Google C++ Style Guide suggests using
signed integers instead of unsigned integers so that
accidentally decrementing a zero value will result in an
easier-to-find fault. Single-byte and four-byte integers
are more common, and they can have range problems. Adding
one to their maximum value makes them wrap around to a
negative value. Therefore we choose edge cases that include
minimum and maximum values, if appropriate, in addition
to $(-1, 0, 1)$.

If a parameter is a vector of numbers, what to test depends
even more on the particular math. Some suggestions:
\begin{enumerate}
    \item Set all values to a single edge case.
    \item Try monotonically-increasing or decreasing.
    \item Try non-increasing or non-decreasing (so that some
    consecutive values are the same).
    \item End points may be special, so test them separately.
    \item If the vector is, in some way, smooth, construct
    a vector where the maximum change, element to element, is no
    more than some prescribed value. See how large that value can
    be before the function fails.
    \item Orthogonal arrays are random draws of values that
    are spread out evenly through a multidimensional space.
    They are one way to explore the space of input arrays~\citep{Owen1992}.
\end{enumerate}
Each of these suggestions becomes another category for
equivalent inputs. We whittle them to manageable numbers of
tests by understanding some of the math and some of the code.


\subsection{Example}\label{sec:parameter-wilson}
This should use Wilson's Score Interval to guess standard error for a measurement~\citep{agresti1998approximate}.

This function finds standard error from effective sample size for two
different models. One is the binomial model, denoted ``proportion,''
and the other is a demographic rate, denoted ``rate,'' which is not bounded
by 1. For the binomial model, the Wilson score interval is
\begin{equation}
    \frac{\hat{p}+z^2/2n\pm z\sqrt{\frac{\hat{p}(1-\hat{p})+z^2/4n}{n}}}{1+z^2/n}
\end{equation}
where $z=z_{\alpha/2}$, so it's .975 for a $95\:\%$~confidence interval.
I think it will translate between confidence interval and standard
error by assuming asymptotic normality, which means assuming the
confidence interval is
\begin{equation}
    p\pm z \times\mbox{standard error}.
\end{equation}
This means a standard error from Wilson's score interval comes from
\begin{eqnarray}
    z \times\mbox{standard error} & = & \frac{z\sqrt{\frac{\hat{p}(1-\hat{p})+z^2/4n}{n}}}{1+z^2/n} \\
    \mbox{standard error} & = & \frac{\sqrt{\frac{\hat{p}(1-\hat{p})+z^2/4n}{n}}}{1+z^2/n}
\end{eqnarray}

For the standard error of a rate, there are two calculations, an asymptotic one
for large effective sample size and another for small effective sample size.
For large sample sizes, standard error is $\sqrt{\lambda/n}$, as expected for Poisson
rates. For small sample sizes, this code uses a linear interpolation between two values,
which are the large sample size value, pinned to $\lambda n=5$, and the value $1/n$.
\begin{equation}
    \left(1-\frac{\lambda n}{5}\right) \frac{1}{n} + \left(\frac{\lambda n}{5}\right)\sqrt{\frac{5/n}{n}}.
\end{equation}
A larger selection of standard estimators for Poisson rates is in \citet{patil2012}.

%https://stash.ihme.washington.edu/projects/CC/repos/cascade_ode/browse/cascade_ode/drill.py
\begin{lstlisting}[language=Python]
def se_from_ess(p, ess, param_type, cases=None,
                quantile=0.975):
    """Calculates standard error from effective sample size
    and mean based on the Wilson Score Interval"""

    err_msg = ("param_type must be either "
               "'proportion' or 'rate'")
    assert param_type in ['proportion', 'rate'], err_msg

    if cases is None:
        cases = p * ess

    if ess == 0:
        return 0

    if param_type == "proportion":
        z = stats.norm.ppf(quantile)
        se = np.sqrt(p * (1 - p) / ess + z**2 / (4 * ess**2))
    elif cases <= 5:
        # Use linear interpolation for rate parameters
        # with fewer than 5 cases
        se = ((5 - p * ess) / ess + p *
              ess * np.sqrt(5 / ess**2)) / 5
    elif cases > 5:
        # Use Poisson SE for rate parameters with more than
        # 5 cases
        se = np.sqrt(p / ess)
    else:
        raise Exception(
            "Can't calculate SE... check your cases parameter")

    return se
\end{lstlisting}

\begin{center}
    \begin{tabular}{ll}
    \hline
    Variable & Possible Values \\ \hline
    mean $p$ & $p<0$, $0<p<1$, $p=0$, $p>1$, single value or array \\
    effective sample size $e_{ss}$ & $e_{ss} <0$, $0<e_{ss}<p/5$, $e_{ss}>p/5$ \\
    param type & proportion, rate, other \\
    cases & missing, None, negative, $<5$, $5$, $>5$ \\
    quantile & missing, $0<q<1$, $q=1$, $q>1$ \\ \hline
    \end{tabular}
\end{center}
The decision table will be quite large, but we can start with some
particular values.
\begin{center}
    \begin{tabular}{llllll}
    \hline
    mean & ess & param type & cases & quantile & result \\ \hline
    0.04 & 100 & rate & missing & 30 & $0.02$ \\
    0.04 & 100 & None & missing & missing & assertion \\
    0.04 & 0 & rate & missing & missing & 0 \\
    (0.04, 0.01) & (0, 0) & rate & missing & missing & (0, 0)
\end{tabular}
\end{center}

This function is written to handle an array of mean values.
If the input mean is an array of numbers and the standard error
is zero, the output of the function is a single zero, not an array
of zeroes.


\subsection{Test case generation}


\subsubsection{Choices of test cases}
There are names for various strategies to generate test cases.
We can discuss those names, but we get to make very detailed
choices about how we generate tests, and testing code tends
to look hybrid, with different choices for different parameters.

It was a choice to decide equivalence classes. Doing a thorough
job can take developer time without offering a lot of improvement
over randomly choosing a parameter value. We can think of the
random parameter value as coming from a single equivalence
class. The values are never
totally random, either. Nobody would test a string parameter with
a random bit value because it would never test for a zero-length
string.

Parameters interact, too. It may be that a computational constant
parameter isn't used at all unless a particular option is chosen.
This can make generating parameters more tricky if they need
to be created together, so a second step, to filter the parameter
sets, can be easier than generating only permitted values.

When we pick a value from each parameter class, we can either
pick a random value within that class or always pick the
same value.

There are a goal strategies that guide these choices.
These all fall under the general heading of test design,
the same kind of test design we use to decide how
to plant test plots of plants, or how to pick gels to
run in a bench experiment.


\subsubsection{Combinatorial excursions}
If you decide that there is some most-common way to call
the function, set that as the baseline. Then loop over parameters,
trying all equivalence classes for each one. Then loop over pairs of parameters. Then sets of three. You define a limiting depth,
where the full combinatorial excursion is the case where the
limiting depth is the number of parameters.

These tests would be highly-biased towards that baseline parameter set.
It can make sense to define several most-important baseline
parameter sets from which to conduct combinatorial excursions.


\subsubsection{Random coverage}

If we use few equivalence classes and choose
random values, that's called random testing. One benefit is
that, the longer you test, the more likely you are to find
a fault. Doubling the number of tests gives you half-as-much
chance of finding a fault. It's common to run some number
of random tests in the regular test suite and occasionally
run tests overnight, just in case. It's important to control
the random number generators for such tests, so that you don't
see a fault once and fail to reproduce it.

The random method is common and effective. It seems impossible
that stratifying parameters using equivalence classes wouldn't
improve testing, but there isn't a record of cases where
it works a lot better than random testing~\citep{arcuri2011random}.


\subsubsection{Single coverage}

Separate parameter values, or combinations of parameter values,
that should cause errors. Test each of those alone.
Then set a goal that one representative value from each
equivalence class shows up in at least one test.
If the largest number of equivalence classes for any parameter
is ten, then this can be ten unit tests.

Choosing so few tests won't cover all lines of code
and will come nowhere near covering all branches in a
large function. Using a single-coverage strategy is about
coverage of the parameters, not coverage of the code.
In practice, we augment where we see risk.


\subsubsection{All pairs}

What if we could guarantee, that if we pick any two parameters
of a function, and we pick two values for those parameters,
that there is at least one unit test that includes that combination
of parameters? This is the all-pairs test design, to test
every possible pairwise interaction among parameters.
It packs these into as few tests as possible. For instance,
the same unit test might be the only time that pair
$(a=1, b=2)$ and pair $(c=3, d=4)$ appear, because their
pairwise interaction appeared in the same test.

All-pairs testing offers a feeling of completeness while
using many fewer tests than even a depth-2 combinatorial
excursion. There are claims that all-pairs testing almost
always finds every fault in a function that full combinatorial
excursions would fine~\citep{Pairwise}. It does not, however,
do any better a job than the randomized testing of emphasizing
particular parameter sets that the author decides carry
the most risk for a function or application.

The trick to using all-pairs testing is finding a library
that will compute the sets of parameters to use. Finding
those sets is, itself, an optimization problem.
While AllPairsPy works for Python~\citep{allpairspy},
and several papers cover algorithms~\citep{tung2000automating,pezze2008},
there are two articles that offer a friendly start~\citep{blass2002,czerwoka2006}.


\subsubsection{Orthogonal arrays}

Picture each parameter as an axis with as many marks
on it as there are equivalence classes for that parameter.
That's a space whose dimension is the count of parameters.
We can choose tests that cover that space evenly, as evenly
as possible. It's the same principle that's used for doing
high-dimensional integration~\citep{Owen1992}.

The trouble with these orthogonal arrays is that they can
be difficult to generate. The algorithms to make them
are complicated and rarely included as programming libraries.
Remember that we can choose different test generation strategies
for subsets of parameters, and orthogonal arrays would be
quite useful for sets of parameters that define a space of
solutions.
\citet{petke2015practical} cover these strategies in their book,
and there are a few survey papers on the topic~\citep{grindal2005,nie2011survey,khalsa2014orchestrated}.


\subsubsection{Automation}

There are testing tools that read your code in order to
generate tests. They then watch your code run those tests,
watching for branches, in order to determine what tests
to create. These dynamic symbolic execution techniques seem
promising and are built into Microsoft's Intellitest.


\subsection{Conclusion}
If you can separate parameter handling from the rest of the code,
you can test better. The idea is to transform from input parameters
that are user-friendly to those that are simpler for the algorithm
to use.
Think of parameter-handling as translation from an external
     language to some kind of parameterization that's more
     consistent, concise for internal processing.




\section{Parallel implementation}\label{sec:parallel-implementation}
\subsection{Write it twice}\label{sec:parallel-twice}
Read through unit tests, and they all have the same form.
They set up data and parameters to pass to a function, call the
function, and either compare its output to some known answer or to
something known about the answer. Every unit test is two
functions, the function under test and a parable
of that function, written into the unit test itself.

Often, we're given code that has some functions that do important
work but have too little documentation and no tests. It's easy
to add tests, in this situation, in order to understand what
those functions do. The tests can then tell the story of how
to set up data for those functions and work with their output.

An easy form of parallel implementation is the previous version
of a function. Given the task to write code, the programmer will
start with the simplest version that does the task, and then they
iterate to make it better. That simplest version makes an excellent
parallel implementation for use case testing.

We can find an appropriate parallel implementation for testing by
looking at the set of choices we make when writing the original function
and asking, at which step should the second implementation differ
from the first.
\cite{dahlgren2005} describe three steps to writing a function for
scientific software, and we'll add one step before theirs.
\begin{enumerate}
  \item Represent the mathematical model.
  \item Prepare discrete models.
  \item Translate the models into algorithms.
  \item Code the models using programming languages.
\end{enumerate}
If only the algorithm in the code changes, then the results
should be quite similar. If the models are discretized differently,
then it takes more analysis to use one for testing of the other.
Which you choose depends on the kinds of faults you think pose
the most risk in the code.

The benefit, and threat to efficiency for testing, is that
parallel implementations of a function can lead to scientific
questions. Good unit testing strategies limit testing to the
highest risks in the software, and that principle applies
especially to use of parallel implementations, so let's
look at how alternative choices probe faults in code.

\begin{callout}
\textbf{The Oracle Problem}---
Imagine that there were some source to tell you what the answer to a function
should be. It could be a software engineering support tool, or it could
be your scratchwork on a notepad. This source is known as an
oracle~\citep{howden1986functional}, and its absence for scientific code
is the oracle problem~\citep{sirer1999using}. Despite the name, this
is hardly a tragedy because testing methods can 
finding simpler inputs, compare results with another implementation,
or check properties of the function without checking the function itself.
\end{callout}


\subsection{Brute force numerical algorithms for comparison}

Let's say we are supposed to calculate the mortality rate
for five-year age groups from birth to eighty years old,
for a Gompertz-Makeham mortality law. The parameters are
$(x, \alpha, \beta, \lambda),$ intermediate values are $(\mu, S)$ and the result is ${}_nm_x$.
\begin{eqnarray}
  \mu(x) & =&  \alpha e^{\beta x} + \lambda \\
  S(x) & = & \exp\left(-\lambda x - \frac{\alpha}{\beta}\left(e^{\beta x} - 1\right)\right) \\
  {}_nm_x & = & \frac{\mu(x+n) S(x+n) - \mu(x)S(x)}{S(x) - S(x+n)}
\end{eqnarray}
Wherever this equation comes from, whether it be a book or a
\textsc{png}\ in a Jira story, we can identify risks in transliterating
these equations to code. The most likely faults are improperly grouping
algebraic operations, mistaking signs, and forgetting terms. 

We can address those risks by returning to the first step of writing
a scientific function, which is how we represent the mathematical model.
The equations above are the result of integrating a function that has fewer
steps, even though it would be slower to run.
\begin{eqnarray}
  \mu(x) & =&  \alpha e^{\beta x} + \lambda \\
  {}_nm_x & = &\frac{\int_{x}^{x+n}\mu(s) \exp(-\int_{x}^s\mu(a)ds)ds}{\int_{x}^{x+n}\exp(-\int_{x}^s\mu(a)ds)ds}.
\end{eqnarray}
The resulting ${}_nm_x$ should be the same as that above within
machine precision, where machine precision means that we expect a drift
of about one machine epsilon for each step of the integration. That means
it should agree within several thousand times machine epsilon.

This new version could have its own faults and its
own corner cases, but they are unlikely to be the same faults
as the original function. This is a tremendous improvement over
not having such a test.

There are many possible parallel implementations. For this example,
we could make the looser assertion that the relative error of ${}_nm_x$
to $\mu(x)$ should be less than some cutoff. It's not as strong a test,
but it's quick to write.

What to test with brute force computation, and how to brute
force that computation, depends on risk assessment, time, and
whether your work is to question faults in the code or faults
in the model.



\subsection{Another language or library}

There is something lovely about the task of rewriting code from
one language to another. On one side sits working code, problems
solved. On the other is a green field for expressing it in the idioms
of a new language. The original code is a gift, not only as a guide,
but as a set of unit tests for the new code, but only for those
willing to take the painful step of limiting their first translation
to perserve structure around the riskiest functions.

Most languages provide some built-in interoperability to call
other languages. R, C++, Julia, and Python play together like hamsters
in a habitrail. It's perfectly reasonable to keep code from
a different language in with the suite of unit tests and to
call those functions when the interoperability is available.

Testing across languages, or in the same language on different
operating systems, can probe a unique set of problems. Data types
change, revealing limits to numerical accuracy and inappropriate
assumptions about data structure. Libraries change, the ones that
provide special functions or system services. For instance, the
generalized inverse in Python will return a different value
from the generalized inverse in R. When this breaks tests, it
clarifies which, of the four kinds of generalized inverse,
the code needed, and that there should be a test to see which
is available.


\subsection{A figure in a paper}
If you read about an algorithm in a paper, and that paper has a table or figure,
you could check your results against the values in the table or figure.
Fig.~\ref{fig:wilson_chart} shows a chart from a paper. The unit test
uses those figures.

\begin{lstlisting}[language=R]
wilson_score_interval <- function(p, n, confidence) {
  z <- qnorm((1 + confidence) / 2)
  fixed <- p + z**2 / (2 * n)
  shift <- z * sqrt(
    (p * (1 - p) + z**2 / (4 * n)) / n
  )
  denominator <- 1 + z**2 / n
  list(lower = (fixed - shift) / denominator,
       upper = (fixed + shift) / denominator)
}

paper_numbers = data.frame(list(
  n = c(
    1, 1, 6, 15,
    1, 6, 7, 13, 3,
    4, 4, 18, 4,
    3, 20, 49, 18,
    13, 59, 16),
  p = c(  # "Total n" column
    1, 1, 0.8333, 0.4667,
    0, 1, 0.4286, 0.5385, 1,
    .5, .75, 0.6667, 0.5,
    1, 0.6, 0.5306, 0.6111,
    0.3846, 0.3898, 0.5),
  w_minus = c(  # "p(shall)" column
    0.2065, 0.2065, 0.4365, 0.2481,
    0, 0.6097, 0.1582, 0.2914, 0.4385,
    0.15, 0.3006, 0.4375, 0.15,
    0.4385, 0.3866, 0.3938, 0.3862,
    0.1771, 0.2758, 0.28),
  w_plus = c(  # "D:w-" column
    1, 1, 0.9699, 0.6988,
    0.7935, 1, 0.7495, 0.7679, 1,
    0.85, 0.9544, 0.8372, 0.85,
    1, 0.7812, 0.6630, 0.7969,
    0.6448, 0.5173, 0.72)
))


test_that("paper numbers match computed", {
  confidence <- 0.95
  result <- wilson_score_interval(
    paper_numbers$p, paper_numbers$n, confidence
  )
  tolerance <- 0.001
  for (i in 1:length(result)) {
    expect(abs(result$lower[i] - paper_numbers$w_minus[i]) < tolerance,
           paste("no match:", result$lower[i], paper_numbers$w_minus[i]))
  }
})
\end{lstlisting}

\begin{figure}
    \centering
    \includegraphics[scale=0.4]{wilson_chart.pdf}
    \caption{A paper about confidence intervals published a chart of values for
    the Wilson score interval~\citep{wallis2013binomial}, in the last two columns. Using $n$ and $p$
    for a $95\:\%$ confidence interval, we can quote this chart and compare with
    our code's output as a unit test.}
    \label{fig:wilson_chart}
\end{figure}

\subsection{Regression tests}
A regression test compares the result of a function against a previous
result from that function. The parallel implementation, in this case,
is the previous version of the same function.

There is a book called \emph{Release It!} that says we should
``make regression testing easy,''~\citep{nygard2018release}. It's hard to make it easy
because unit tests are built to test, not save output. Even if
they save output from functions, it takes work to figure out
\emph{how} a result changed if it did change. If the result
is a data frame, did every value change, or some subset?

While following the book's advice is a research project,
it's a good focus for any particular project to find places in the
code where regression tests would be most informative. Unit tests are
most absolute in their pronouncement when they test the simplest
functions. It's the mid-level functions, that do more work
and produce the more complex output, that benefit from regression.
The high-level functions will change output every time you touch
the code, but mid-level functions will change less frequently,
and there's a chance to guess why they changed.


\subsection{Checkout testing}\label{sec:test-compiler}

Let's say you have a cluster of computers. They are all Intel machines,
but they come from different years. They could be different architectures
or just different steppings of the \cpu, which is the least difference
one can have between two chipsets. You compile code on one machine,
or you install some \rlang package that compiles code underneath. When
you move to another computer on another day, that same code fails
to run with the message, ``invalid opcode.''

An opcode is a machine-language instruction. The computer you used
yesterday knew a command that's slightly too fancy for the computer
you're using today, and it failed. You have to clean out the code
and recompile today. It's not that the new computer is just older.
It's still possible that, were you to switch back, there might be
the same message again. The history of \cpu opcodes isn't a forward
march into a larger, better set.

That scenario would be annoying, and it's a headache for people
saddled with heterogenous clusters. They solve it with, for instance,
creating architecture-specific shared drives so that they can
install a different version of the software for each version of
system architecture on the cluster. That scenario is, however, a
best case scenario.

The worse scenario, and I wish this were rarer, is that the
software runs on the other computer but gives the wrong error.
It can fail silently. This seems like an abrogation of a compiler's
contract with its faithful coder, but it isn't. That compiler
ensured that the code ran well enough on one target architecture.
The problem is that compiler optimizations at level \textsc{o3}, and above,
are willing to make slightly riskier choices that yield great
performance benefits. These choices are tuned to the architecture
at hand, or as requested by compiler options, and, while they may
run on similar architectures, carry more risk of error on them.
It solves the problem of having languages
that don't clearly represent time-saving invariants and of coders
who can't afford motorcycles.

There's a solution for this, too. Turn all optimizations down
to \textsc{o2} or lower. Or claim to the compiler that the \cpu
understands no later than the \textsc{sse2} instruction set from 2000.
Unless you're intentionally commemorating the year Phil Katz died, you'll see
this as a severe loss of performance.

The last option to deal with this problem is called checkout,
which is ``testing conducted in the environment to ensure that a software product performs as required after installation. (IEEE 829-2008)'' It means that you create a test to run before you
run code on a new machine. It won't do the work of creating
a separate version of the code, if needed, but it tells you if
you need it. You don't generally care about integer performance
here. You care about vectorized code and floating-point code.
That's where the problems will be, so you can subset the test
suite for these acceptance tests.

Designating a subset of all tests as acceptance tests is an
example of test case prioritization~\cite{rothermel1999test}.
We prioritize tests by hand, by marking tests to be run under
different conditions, but tests for scientific code can be lengthy,
which makes it worth considering automatically reordering tests
so that those that are most sensitive to correctness run by 
themselves, or run first~\cite{yoo2012regression}.

\section{Testing traits of a function}

\subsection{Continuity}

Another kind of limit is the distance between function results
as the function inputs get closer to each other. We can often
find faults in a function by testing continuity of the output.
Here, a good unit test would mirror the definition of Lipshitz continuity.
\begin{lstlisting}[language=python]
C = 2.0
for (x1, x2) in [(0, 1), (0, .1), (0, .001)]:
    assert f(x2) - f(x1) < C * (x2 - x1)
\end{lstlisting}
The \lstinline!C! variable is a bounds on the slope for any
pair of numbers.

Sometimes a limit of a function is the same limit you know
from high school precalculus,
\begin{equation}
  \lim_{x\rightarrow\infty} f(x).
\end{equation}
Maybe the limit goes to zero, or to 3, but the principle is
the same. You can test that a function is near the limit
as the value gets larger, or closer to zero. Better, you can
test that the function gets closer to the limit as the
parameter gets closer to its limit.

\subsection{Approach to limiting values}

This is a game where incremental understanding of math
gives incrementally more careful attention to risk in the
function under test. We can know the limiting value of a
function, but we can also know how the function approaches
that limit. Another precalculus friend, the Taylor series
writes a function as a sum of terms that get smaller,
\begin{equation}
  f(x_0+x) \approx f(x_0) + \frac{f'(x_0)}{2}x + \frac{f''(x_0)}{6}x^2 + \epsilon.
\end{equation}
If we know the first term in the series, then the second term
is a good guess for how far the function's output should be from
the limiting value, as a function of $x$.


\subsection{Using symmetry to test a function}\label{sec:symmetry-test}

Sometimes knowing the math of a software function lets you
test the software, even when you have an oracle problem.
\emph{Metamorphic testing} uses the symmetry of the math function
in order to test the correctness of the software function~\citep{ding2016,guderlei2007,kanewala2015,liu2014}.

The mathematical description of a function is that it relates
a domain (input) to a range (output). We write $f(x) = y$.
If the function is an exponential, then $e^{-x} = 1 / e^x$,
so $f(-x) = 1 / f(x)$. We may not know what the value of
$e^{2.4}$ is, but we do know it has to match $1 / e^{-2.4}$.

Here's a longer example. Given a mortality rate, $m$, the
mean age of death over an age interval, $n$, is
\begin{equation}
    a = \frac{n - n(1+m n)e^{-m n}}{1 - e^{-m n}}.
\end{equation}
If we notice that this equation can be written
\begin{equation}
    a = f(m, n) = n  f_1(m  n),
\end{equation}
then we can design an interesting test.
\begin{eqnarray}
    \frac{a}{n} &= &f_1(m  n) \\
    \frac{a/2}{n/2} &= & f_1((2  m)  (n / 2)) \\
    a /2 &= & (n/2) f_1((2  m)  (n / 2)) \\
    a &= &2 * f(2m, n/2) \\
    f(m, n) & =& 2 * f(2m, n/2)
\end{eqnarray}
This result isn't true for almost all functions. It's a strong test
of the implementation. It's an example of the more general
form of symmetry testing, which insists that some transformation
of the inputs has a relationship with some transformation of the 
outputs,
\begin{equation}
    f(g(x)) = h(f(x)),
\end{equation}
where $f$ is the function under test, and $g$ and $h$ are the transformations.

\begin{lstlisting}[language=R]
mean_age_of_death <- function(m, n) {
  (n - n * (1 + m * n) * exp(-m * n)) / (1 - exp(-m * n))
}
test_that("dividing by a constant multiplies by a constant", {
  m1 <- 0.01
  n1 <- 1.0
  a1 <- mean_age_of_death(m1, n1)
  for (k in seq(0.1, 5, 0.3)) {
    a2 <- mean_age_of_death(k * m1, n1 / k)
    expect_lt(abs(2 * a2 - a1), 1e-7)
  }
})
\end{lstlisting}

Metamorphic testing is an assertion about a symmetry relationship between
the domain and range of a function. Because it's an invariant at the top-level
of the function, it has an opportunity to find simple faults in algebra,
grouping of terms, or off-by-one errors. It also has an opportunity to
find faults that result from denormalization or loss of significant digits
in floating-point math.



\section{Statistics for testing}\label{sec:statistical}

\subsection{Randomized calculations should give different answers each time}

A simple Gaussian elimination can benefit from random pivoting.
A function to sample a distribution is supposed to return a new value
every time it's called.
Some functions introduce random reorderings
so they can insist that the caller not expect a given order
in the output. All of these perfectly useful functions
thwart using simple lookup tables to test their output.

The tests, themselves, can be the problem when we randomly
generate test cases. I'll check
a life expectancy function against a corpus of life expectancies
for every country. I'll randomly generate user input
to see that a functions succeeds or gracefully fails.

When testing these random functions,
we have on our side some basic tools
of statistics. Some, like mean square error, are ways
to heap our uncertainty into a single, tidy number.
Others are formal statistical
tests. They take a clear side-effect of a function,
its randomness, and tame it to measurable uncertainty.
Let's look at just a few examples.

\subsection{Approaches}

\subsubsection{Error estimation}
When the function under test returns a vector of values,
and I want to compare it with a vector of known values,
I use an estimator, like mean-squared error, of the difference
between the two.

The mean-squared error between $N$ observed values $\hat{Y}$ and $N$ known values $Y$ is $\frac{1}{N}\sum_i (Y_i - \hat{Y}_i)^2$.
It takes the whole set of values and reduces it to one number I can assert must be less than some value.

When values range from small to large, it's convenient to use
relative error instead of absolute error.
\begin{equation}
  \mbox{relerr} = \frac{\hat{Y}_i - Y_i}{Y_i}
\end{equation}
The known value is always in the denominator.

Even more reassuring for a unit test, I can take all of the
sets of values and take the maximum absolute error,
$\mbox{max}_i |Y_i-\hat{Y}_i|$. That's a guarantee there isn't
some boundary value whose crazy-high error is hidden beneath
low error everywhere else.

\subsubsection{Variance, standard deviation}
When there is just one vector of random numbers coming out
of a function, it can be helpful to characterize how far
they are from their average value. The variance is
the average of how far each value is from the average of all
values,
\begin{equation}
  \mbox{var}(Y) = \frac{1}{N}\sum_i (Y_i - \bar{Y})^2,
\end{equation}
but you'd use the always-avaiable
as \lstinline!var! or \lstinline!variance! in any language.
The standard deviation is the square root of variance.

If variance is zero, then every value in the vector is
the same. There are some functions, such as the Cauchy
distribution, that don't have a variance. For such a function,
the value of the variance will keep increasing as the
number of draws of the function increases, instead of
converging to a single value. This, in itself, is a
testable property of a function.


\subsubsection{Confidence interval}

Assume there is a function with randomness, that produces floating-point numbers. Those results
have some average value, but they vary around that average value.
If we run this function a thousand times and take the
lower value, below which 2.5\,\% of the results fall,
and the upper value, above which 2.5\,\% of the results fall,
then this interval is the 95\,\% standard deviation.

For research statistics, the standard deviation is often part
of an argument that a model of a problem is appropriate to
the observed data. For unit testing the standard deviation
is a rule of thumb for how often a perfectly good unit test
will randomly fail. If you want it to fail five percent of
the time, use a 95\,\% interval. If you want it to fail
one time in a thousand, use a 99.9\,\% interval.

Given that results from a function are random, and we
want to make some cutoff on which results are acceptable,
we want to choose a cutoff that is informative, so it's
close to the average good value. On the other hand, we don't
want it to fail a lot. The standard deviation tells us
how much push luck in either direction.

\subsubsection{Pearson coefficient}
Let's say the output of a function is a set of $y$ values,
and the input is a set of $x$ values. You may not know
how $y$ depends on $x$ exactly, but you know that it
does. The Pearson coefficient is a function of
$N$ $y$ values and $N$ $x$ values that will return a number
between -1 and 1, where -1 is a line with a downward slope,
1 is a line with an upward slope, and 0 means the points
are evenly arranged so that they don't slope up or down
and aren't collinear.

For a single $y$ that depends on a single $x$, the equation
is a few lines of code.
\begin{equation}
r_{xy} = \frac{\sum_i (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_i (x_i - \bar{x})^2}\sqrt{\sum_i(y_i - \bar{y})^2}}
\end{equation}
It conveniently generalizes to more dimensions.

The Pearson coefficient is a handy, if imprecise,
set of guard rails on the behavior of a function.
It quickly determines whether the output rises or falls with the input,
even in the presence of noise. It can say whether the output
isn't linear enough, or whether it's too linear.
It even has a multidimensional generalization for data that
has multiple $x$ and $y$ values.


\subsection{Taming randomness}

I make tests with random inputs, so they explore more alternative
paths in the code than I might guess. I run tests of random
functions a thousand times so that they have more chances to
show a real bug. Randomness in testing is a free source for
finding faults, and it's a constant headache.

In order to use the statistical tests above, I need to choose
a cutoff on what is a passing test and what is a failing test.
If I choose a cutoff that's useful for finding faults, it will
always fail for some percentage of tests. For a 95\,\% confidence
interval, it will fail 5\,\% of the time. Even for a
99.9\,\% confidence interval, it will fail once in a thousand
tries. This means the green lights of a unit test display will
be red without reason once in a thousand times, but that's
enough not to pay attention to a red light. It's a lure to
rerun the test suite every time there is any red light.
It's a waste of attention.

The solution is simple. Fix the seed for the random number
generator for each test that has randomness. The random
seed determines the exact set of random values that will
follow. If the test passes for a particular random seed,
then it will always pass for reruns of the unit test until
some fault is introduced.

The simple solution is sometimes impossible and always
disappointing. Most programming languages offer the tremendous
convenience of calling \lstinline!rand()! anywhere in the
code, which uses a global random number generator. Sometimes
\rlang code calls \lstinline!rand()! and then calls \cpp code
that calls \lstinline!rand()! from a different random number
generator. Authors
of code unfortunately use this convenience, creating side-effects
for testing. Only sometimes can we make a repeatable
stream of random numbers by declaring a specific random number generator
and passing it to a function.

(It's also possible to greatly reduce the rate of failure
for a statistical test, even with the same 99\,\% cutoff
on acceptable results, if we take the same hundred tests
and use the statistical framework to treat them as ten sets
of ten tests.)

Let's say we can fix the seed for a function, and for all the
functions it calls. The joy of randomized testing is that more
runs might yield more faults in the code. We
can still do lots of runs by creating a unit test that is run once
for a set of seeds.  The procedure is to define a starting seed and run against
the next thousand seeds. When the test shows no real faults,
choose a starting seed for which the next thousand all pass
the test within some tolerance. This is less simple, but it
explores the space of tests and is likely to signal when
a real fault arrives.

\subsection{Conclusion}

Some kinds of unit tests are absolute. When they pass,
there is no fault in this one path through the function.
Statistical tests don't give that guarantee. The function
under test could be close-enough to the desired value but
still have a fault in some code path.
At the same time, statistical tests can find faults
in places you weren't looking, because they summarize all
of the outputs of a function or because they summarize
the behavior of multiple runs of a function.

If we were testing a typical function, for which we knew the
right answer, that unit test would offer the meager assurance that,
for one set of parameters, the function is correct.
Statistical tests, like those above and many more statistical
tests not mentioned, can assert that, for one set of parameters,
a function isn't too wrong.





\section{Testing data movement}\label{sec:data-movement}

\subsection{Data movement}\label{sec:movement-movement}

We said in Sec.~\ref{sec:parallel-twice} that there are four
steps to writing a scientific function, representing the mathematical model,
preparing a discrete version, translating it to algorithms,
and writing the code.
Rarely is scientific software just one mathematical model
and just one ensuing algorithm. It's a series of models supported
by a series of algorithms.

For this software, as for any software, using the right algorithm
ensures speed and scalability, and the right algorithm depends on
its associated data structure. The relevant undergraduate class
is called Data Structures and Algorithms, always the two together.
As a result, performant scientific code is a series of algorithms
joined by a series of translations from one data structure to
another, each to support the next algorithm. The high-performance
computing crowd calls any such code \emph{data movement,} as though
it were replacing sets in the entracte,
but software engineers call it integration code and know that
integration testing is one of the most important kinds of unit test.


\subsection{Integration testing data movement}

A classic example of data movement is reorganizing a multidimensional
array from one axis ordering to another. An algorithm with nested
for-loops usually acts on nested axes. The order of the axes should
put those for the inner loops last (for \textsc{c} and Python which are row-first), or put those
same axes first (for \rlang and Julia which are column-first). This so
often controls the speed of an algorithm that it isn't considered an 
optimization to fix it but an error to fail to set the order.

More complicated an example would be translating a graph from
an edge-list representation to an adjacency matrix representation.
Both have their place, but the correct form depends on the
algorithm that will act on this data.

Data movement tends to stress performance of memory access
and integer arithmetic. It carries risks of off-by-one errors,
loop endpoint errors, and assignment errors. These are different
from the performance characteristics and likely faults in core
algorithmic code, so they suggest data movement should be in its
own function and should be tested separately, for both performance
and efficient testing.

This produces a mental model for a scientific application. It begins
with translating configuration parameters from their specification
to the values internally. Inputs are verified. Then the application
is a series of data movement, followed by algorithm, and data movement
for the next algorithm. Then results are written.

\subsection{The CRUD case}\label{sec:crud}

The prosaic brother to data movement is the ubiquitous selection
and editing of data frames when reading and writing data.
The data frame is the algorithmic equivalent of a minivan, more
about convenient pickup and drop-off than about getting from $A$
to $B$. When we edit data frames, during save or load, or during
data movement, the correct result is more a matter of software specification
than a matter of difficult calculation.

A good testing method for data frame manipulation, is therefore
unit tests that assert the output data frame meets a list of requirements.
There is, happily, a framework
for recording and conveying this work, the create-read-update-delete
(CRUD) list. For example, if we were editing a mortality data frame:
\begin{itemize}
\item \textbf{Create} - If the input data doesn't have excess mortality,
  then derive it from cause-specific mortality and prevalence.
\item \textbf{Read} - If the user specifies \lstinline!get_csmr!, then get cause-specific
  mortality from the database and add it to the data.
\item \textbf{Update} - Use the average of incidence data at all child
  locations because it tends to be sparse.
\item \textbf{Delete} - All-cause mortality isn't included in this step.
\end{itemize}
These are requirements. Failure to meet one of these requirements
often won't make the code stop running. It's no less wrong.

These tests are classic boundary-value tests, where
we try parameters at the lower boundary, upper boundary,
and middle of the range. Combinatorial testing can be appropriate
across all parameters, and we can often create a single data frame
where each row represents a different test.

\subsection{Example}
This example, in Python, is a function with two arguments, but both
are dataframes, just read from input files. Think of them as two spreadsheets
with multiple rows and column headers \lstinline!x_local!, \lstinline!a_data_id!,
and so on.
\begin{lstlisting}[language=Python]
def shift_priors(self, data, adj_data):
    """For subnationals, shift priors given parent run.

    Args:
          data (pd.DataFrame): All of the input data, including priors.
          adj_data (pd.DataFrame): Parent predicted data values.

    Returns:
        Data with adjusted priors.
    """
    integrands = data.integrand.unique()
    priors_from_draws = data[data.x_local == 0]
    unadjusted_data = data[data.x_local == 1]
    parent_predict = adj_data[
        (adj_data.x_local == 1) &
        (adj_data.a_data_id != IHME_COD_DB) &
        (adj_data.location_id == int(float(self.loc)))]
    shifted_priors = pd.DataFrame()
    for ig in integrands:
        ig_prior = priors_from_draws[priors_from_draws.integrand == ig]
        ig_data = parent_predict[parent_predict.integrand == ig]
        if len(ig_prior) == 0:
            pass  # No priors to adjust
        elif len(ig_data) == 0:
            # No fit with which to adjust, so keep the priors.
            shifted_priors = shifted_priors.append(ig_prior)
        else:
            ig_prior -= ig_data.mean()
            shifted_priors = shifted_priors.append(ig_prior)

    shifted_priors['meas_value'] = shifted_priors.meas_value.clip(lower=0)
    newdata = unadjusted_data.append(shifted_priors)
    return newdata
\end{lstlisting}

If we wanted to make a decision table for this function, what are
the possible values of each entry in each row of the input data?

\begin{center}
\begin{tabular}{|l|l|}
\hline Variable & Values \\ \hline
\lstinline!data.x_local! & 0, 1, not 0 or 1 \\
\lstinline!data.integrand! & same as another row, same as adjusted row \\
\lstinline!adj_data.x_local! & 0, 1, not 0 or 1 \\
\lstinline!adj_data.a_data_id! & \lstinline!IHME_COD_DB! or not \\
\lstinline!adj_data.location_id! & \lstinline!self.loc! or not \\
\lstinline!adj_data.integrand! & same as another row, same as data row \\ \hline
\end{tabular}
\end{center}

\noindent{}That makes $3\times 4 \times 3\times 2\times 2\times 4=576$ different inputs.
This excludes the effects of calling the function
\lstinline!adjust_prior_by_mean_of_fit()!.

What is the algorithmic complexity of this function? There is one
if--then--else, so the answer could be three, but each condition in a dataframe
selection is another path through the code. For instance,
\lstinline!data.x_local == 0! is a conditional with two outcomes, leading
to very high algorithmic complexity, hidden in data frame arguments.

A project manager can ensure good communication between the scientist and
developer by writing \textsc{crud} assertions in English.
\begin{itemize}
    \item[R1.] Read parent location's estimate of priors as ``adjusted data.''
    \item[U1.] Update adjusted data so that the mean of the priors for this location matches
        the mean of the data for this location. Keep the shape of the priors.
    \item[U2.] Update priors, after adjustment, so that they are greater
        than zero for all integrands.
    \item[D1.] Delete local adjusted data that doesn't match the current location.
    \item[D2.] Delete local adjusted data that doesn't match the integrands.
    \item[D3.] Delete local data for this location if there are no parent
        priors for the same integrand.
\end{itemize}
Then we can write tests that verify these \textsc{crud} requirements for
each of the 576 kinds of inputs.





\section{Outside the code}\label{sec:outside-code}

\subsection{Process}

The most important non-functional qualities in research code are that it
is transparent and changeable. We want to be nimble and expressive
with our coding experiments. By the time this research code
becomes more established, we think more about ensuring efficiency
and that the code be usable. It should support optional ways to
call it. Somewhere in-between the birth of a new function and its
adoption into our library of code, we find ways to ensure its
correctness.

These conflicting priorities create tension in priorities.
More testing makes code more brittle. Early work is about
experimentation, and later work is about confirmation and
expansion of capability.


\subsection{Documentation defines what a function wants to be}

Sec.~\ref{sec:parameter-wilson}, on parameter testing, uses a function that's a hybrid
between Wilson's score interval and a Poisson estimator. That hybrid
doesn't have a name, and there's no paper or Wikipedia entry for it.
We can test that its output is a continuous function of its input
parameters. We can test that it matches known values for limiting cases,
but the right version of this function is the one that a team agrees
is right.

That team is usually not just the people in the room. It includes stakeholders
outside the room. It includes reviewers. It includes future versions of the team.

The tests can be a clear statement of the contract this function upholds.
So, too, can project documents, as mentioned in the data movement section,
where \textsc{crud} defines selection from a dataframe. 
Function documentation, in particular, is where the software
developer, the one who wrote the function, states their understanding
of the contract that function fulfills. It's the closest statement,
in English, of what unit tests should assert.

Like all other work, documentation takes time and is prioritized by
how it addresses risk. Give the highest-level explanation first.
This could be a reference to a paper, a book section, documentation from another library.
Then say \emph{why} this function exists and \emph{why} it works the
way it does.

And on the technological side, equations in documentation communicate
clearly with research-oriented readers. Similarly, code examples, run
as unit tests themselves, within the documentation, ensure always-working
examples for development.


\subsection{Team is critical to tests}

Take a function under test and project it onto a
screen at the front of a meeting room. Place around that screen
the team you have or the team you would like to have.
Engineering studies show that the future utility, or burden,
of that function depends no more on its testing than it
depends on the culture of that team~\citep{neumann2016,kanewala2014}.

Culture is beyond the scope of this document, but conversations
aren't. One point to raise in that meeting room could
be the effect of discretization or floating point representation
on the mathematical representation. Another could be a
description of statistical properties of the function, a set
of equations, beyond those to implement the function, that
describe its expected behavior. Someone could raise an observation
that brute force solution is suprisingly possible.
The team could discuss likely sets of parameters that might
matter, some determined by the math, some by logic in the
code. A product manager could supply \textsc{crud} requirements.
A systems engineer could describe the expected architectures
and risk of failure due to optimization levels. Every section
of this document is a prompt for conversation.

Testing well is helpful, but we saw, in the section about
data movement, that identifying what is data movement
and what is an algorithm meant we could label the two.
Giving a function a good name is important to a software
developer, but scientific code is rarely owned by a team
of software developers. It's shared by a team of researchers.
That team has a body of knowledge about the software, and,
the same as any body of knowledge, it's a shared set of
understanding where new things are added and less useful
ideas are dropped.

In this research environment, collaboration on testing
creates commonly-understood, trusted functions. This shared
agreement on the (tested) entities with which we do our work, more
than any single snapshot of the whole calculation, are
what the group understands about the code. These trusted
functions will be copied into other projects, translated
to other languages. They will exist long beyond the work
that has your name.

\section{Further reading}\label{sec:further-reading}

The most complete books on general software testing offer a sound
basis for testing any kind of code. I recommend starting with
\emph{Software Testing: A Craftsman's Approach,}~\citep{jorgensen2013},
while skipping its chapter on set theory, and then taking a look at
\emph{Software Testing and Analysis: Process, Principles, and Techniques,}~\citep{pezze2008},
if you haven't had enough.

The complexity of scientific code not only makes it difficult
to test but also mummifies past working code. Lest such a mummy
come to life in your hands, look at \emph{Working Effectively with Legacy Code,}~\citep{feathers2004working}.

There are several sources directed at scientific developers, for
construction of code, as well as testing. These are some recent highlights:
\begin{itemize}
	\item \emph{Software Engineering for Science,}~\cite{carver2017}.
	\item ``Testing Scientific Software: A Systematic Literature Review,''~\cite{kanewala2014}.
	\item ``Development of Scientific Software and Practices for Software Development: A Systematic Literature Review,''~\cite{neumann2016}.
\end{itemize}

\bibliography{tsci}
\end{document}
