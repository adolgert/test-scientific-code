

I have written unit tests without thinking about how I chose
what to test. Foremost in my mind is risk, which I estimate
from how likely something is to go wrong, how bad it will be,
and how hard it is to mitigate. That's the first two steps
of choosing unit tests.



Often, we're given code that has some functions that do important
work but have too little documentation and no tests. It's easy
to add tests, in this situation, in order to understand what
those functions do. The tests can then tell the story of how
to set up data for those functions and work with their output.



Methods to test parameter logic all have the same underpinning,
that they will try to test at least one set of parameters from
each of the equivalence classes of inputs. Deciding what is an
equivalence class can be difficult. Sometimes two parameters
sets are equivalent in terms of the mathematical statement of 
the model, but their representation in code makes one set
have a different risk from another. For instance, a function of small
values would be well-defined as an equation, but it could cause
denormalization when written in code.
It can also be the other way around, that
the mathematical statement of a model makes clear what parameter
sets may be equivalent. Information from both statements of
a function will be helpful.

Not every parallel implementation is a brute-force retelling
of a model. Most mathematical functions provide finger-holds
support of their correctness.

The simplest of these are parameters for which a function
takes a known value. The function could be a mess of
polynomials of trigonometric quantities,
\begin{equation}
  y = (\sin x)^3 - 6 (\cos x)^2 + 2,
\end{equation}
but these simplify considerably at $x=0, \pi/4, \pi/2, \pi,$
and so on. If the function should divide by zero for some
number, then that, too, is a parameter for which the function
has a known value, an exception.


\subsubsection{Checkout testing}




Let's say you have a cluster of computers. They are all Intel machines,
but they come from different years. They could be different architectures
or just different steppings of the \cpu, which is the least difference
one can have between two chipsets. You compile code on one machine,
or you install some \rlang package that compiles code underneath. When
you move to another computer on another day, that same code fails
to run with the message, ``invalid opcode.''

An opcode is a machine-language instruction. The computer you used
yesterday knew a command that's slightly too fancy for the computer
you're using today, and it failed. You have to clean out the code
and recompile today. It's not that the new computer is just older.
It's still possible that, were you to switch back, there might be
the same message again. The history of \cpu opcodes isn't a forward
march into a larger, better set.

That scenario would be annoying, and it's a headache for people
saddled with heterogenous clusters. They solve it with, for instance,
creating architecture-specific shared drives so that they can
install a different version of the software for each version of
system architecture on the cluster. That scenario is, however, a
best case scenario.

The worse scenario, and I wish this were rarer, is that the
software runs on the other computer but gives the wrong error.
It can fail silently. This seems like an abrogation of a compiler's
contract with its faithful coder, but it isn't. That compiler
ensured that the code ran well enough on one target architecture.
The problem is that compiler optimizations at level \textsc{o3}, and above,
are willing to make slightly riskier choices that yield great
performance benefits. These choices are tuned to the architecture
at hand, or as requested by compiler options, and, while they may
run on similar architectures, carry more risk of error on them.
It solves the problem of having languages
that don't clearly represent time-saving invariants and of coders
who can't afford motorcycles.

There's a solution for this, too. Turn all optimizations down
to \textsc{o2} or lower. Or claim to the compiler that the \cpu
understands no later than the \textsc{sse2} instruction set from 2000.
Unless you're intentionally commemorating the year Phil Katz died, you'll see
this as a severe loss of performance.

The last option to deal with this problem is called checkout,
which is ``testing conducted in the environment to ensure that a software product performs as required after installation. (IEEE 829-2008)'' It means that you create a test to run before you
run code on a new machine. It won't do the work of creating
a separate version of the code, if needed, but it tells you if
you need it. You don't generally care about integer performance
here. You care about vectorized code and floating-point code.
That's where the problems will be, so you can subset the test
suite for these acceptance tests.

Designating a subset of all tests as acceptance tests is an
example of test case prioritization~\cite{rothermel1999test}.
We prioritize tests by hand, by marking tests to be run under
different conditions, but tests for scientific code can be lengthy,
which makes it worth considering automatically reordering tests
so that those that are most sensitive to correctness run by 
themselves, or run first~\cite{yoo2012regression}.





\subsubsection{Stratification and filtering}\label{sec:parameters-stratification}
The sections above all imply that there could be some way
to emphasize which test are chosen, based on an assessment
of which parameters carry the most risk. For instance, if
a parameter is an option to make a copy of the input data
before further work, it would be enough to test that exactly
once, and not in all other parameter combinations. On
the other hand, there may
be certain parameter values that should be thoroughly-tested
because they challenge the numerical precision of the algorithm.

The ability to prefer tests of certain sets of parameter values
is called \emph{stratification.} That means you take any
of the test generation methods above and give the test author
the ability to weight which tests it should generate.

The throw out some of the generated tests is called
\emph{filtering.} Sometimes it's easier to generate tests
and toss useless ones than it is to generate the most-desired
tests in the first place.

Some of the more professional testing software includes
these capabilities, such as \textsc{aetg}~\citep{cohen1997aetg},
\textsc{cts}~\citep{hartman2004problems},
and the \textsc{act} library~\citep{kuhn2008automated}.


\subsubsection{All other methods}
The orthogonal arrays, mentioned in Sec.~\ref{sec:parameter-decision},
are yet another way to generate tests that span the space of
all possible parameters~\citep{Owen1992}.
Whichever method you choose, it's
good to have one of these methods available when it's time to
do user-level testing on scientific code.
\citet{petke2015practical} cover these strategies in their book,
and there are a few survey papers on the topic~\citep{grindal2005,nie2011survey,khalsa2014orchestrated}.



Our attention shifts to asking, which parameter sets should I test
for this function? Which parameter sets are meaningfully different
test? How do I know when I've tested enough of them?


\section{disaggregating function}

We could be testing a function where we know what answer it should
give. For instance, take a function which disaggregates its input
vector into finer bins.
\begin{lstlisting}
function disaggregate(coarse, coarse_bins, fine_bins)
\end{lstlisting}
For the unit test, we can create a smooth-enough function on
the fine bins, aggregate it to the coarse bins, and use
that as our known input.


\section{Faults in Mathematical Code}\label{sec:faults-and-failures}

For each section, ask what faults does this technique limit?
For instance, using a limit test for constant-mortality mean age
ensures you don't have a numerical fault.

\subsection{Fault versus Failure}
Jorgensen's \emph{Software Testing: A Craftsman's Approach} distinguishes
faults from failures~\citep{jorgensen2013}.
A fault is the characters that were typed
incorrectly. A failure is observation that something went wrong.
For instance, there may be a fault that 0.3 is assigned to an integer
instead of assigning it to a double. The failure will happen later,
in the code that divides $1 / 0$.

Any failure is a problem. We don't want to throw exceptions or segfault.
Worse for mathematical code is when an error in the code isn't visible
but does give a wrong answer.
The worst problem is faults that aren't failures.


\subsection{Classes of Faults}
Jorgensen gives a set of fault categories and examples, elided here:
\begin{itemize}
    \item Input/Output - incorrect input accepted, incomplete result
    \item Logic - missing cases, test of wrong variable
    \item Computation - incorrect algorithm, parenthesis error
    \item Interface - parameter mismatch, call to wrong procedure
    \item Data - Incorrect type, wrong data, off by one
\end{itemize}
These all apply and, yes, read Jorgensen, but most of this article 
expands upon a single category of fault: incorrect algorithm.

This document covers two different kinds of failure-free faults.
\begin{itemize}
\item Simple data manipulation that doesn't do what the user
   expected it to do.
   
\item Mathematical functions that return a result that looks
   reasonable but isn't correct.
\end{itemize}
While these faults are within the taxonomy of faults
in any book on testing, we can discuss some of the particular
challenges that complicated mathematical functions introduce.



\section{Statistics}

Our tools to
understand and interpret this experimental data fall into a
few categories of \emph{statistical} tests.
\begin{itemize}
	\item A statistical test that a measure of the data matches
	      a known value. We could ask whether the mean of the durations
	      is what we expect from the construction of the function.

	\item A statistical test of differences between two experiments.
	      This could be a comparison of outputs from parturition
	      for two different cattle breeds.

	\item A statistical test to characterize the shape of an
	      association. For instance, does the duration between
	      births increase as weight increases?
\end{itemize}
For each of these kinds of statistical tests, there are a range of
standard methods. We choose among those methods according to what
our goals are for particular software tests.


\begin{itemize}
	\item Develop new code.
	\item Unit test that code to ensure it isn't broken by a change.
	\item Regression test when writing a new version of that code,
	      either for refactoring or for a modification in function.
	\item Acceptance test, or checkout test that code.
\end{itemize}